{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Script for running custom 5 fold cross-validation for VAE-MLP model\n",
    "### running the best 5 VAE hyperparameters and the best 10 MLP hyperparameters for each fold\n",
    "### selecting the best SSIM VAE and best test AUC MLP model for each fold\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a06b5057d479f6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch for data loading and for model building\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, balanced_accuracy_score, confusion_matrix\n",
    "from scipy.spatial.distance import cdist\n",
    "import nibabel as nib\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from PIL import Image\n",
    "from pylab import rcParams\n",
    "\n",
    "# Maths\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# Other\n",
    "from time import perf_counter\n",
    "from collections import Counter,OrderedDict\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "# import functions from other files\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from models.VAE_2D_model import VAE_2D\n",
    "from models.MLP_model import MLP_MIL_model_simple, MLP_MIL_model2\n",
    "from utils.datasets import Load_Latent_Vectors, LoadImages, prepare_VAE_MLP_joint_data\n",
    "from utils.utility_code import plot_results, parameter_count, get_single_scan_file_list, weights_init, get_class_distribution, plot_MLP_results, error_analysis\n",
    "from utils.train_and_test_functions import train, test, train_VAE_model, evaluate_VAE, mixup_patient_data, mixup_batch, process_batch_with_noise, calibration_curve_and_distribution\n",
    "from utils.loss_functions import loss_function, kl_annealing\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c775424afb9ffa0"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 200.0, 'threshold': 0.4357499999999999, 'num_synthetic': 22.5, 'oversample': 1.5, 'batch_size': 118.16666666666667, 'lr': 0.006591666666666666, 'weight_decay': 0.16217499999999999, 'accumulation_steps': 4.25, 'patch_hidden_dim': 1920.0, 'patient_hidden_dim': 98.33333333333333, 'patch_dropout': 0.3708333333333333, 'patient_dropout': 0.25833333333333336, 'alpha': 0.775, 'mixup': 0.3333333333333333, 'attention_indicator': 0.4166666666666667, 'max_node_slices': 20.416666666666668, 'device': 0.0}\n"
     ]
    }
   ],
   "source": [
    "device = 0\n",
    "vae_params = [\n",
    "    {'num_epochs': 200, 'threshold': 0.5, 'num_synthetic': 25, 'oversample': 1.5, 'batch_size': 128, 'lr': 0.006, 'weight_decay': 0.14, 'accumulation_steps': 4, 'patch_hidden_dim': 2048, 'patient_hidden_dim': 128, 'patch_dropout': 0.35, 'patient_dropout': 0.3, 'alpha': 0.8, 'mixup': False, 'attention_indicator': True, 'max_node_slices': 15, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border', 'patient'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.509, 'num_synthetic': 25, 'oversample': 2, 'batch_size': 100, 'lr': 0.00621, 'weight_decay': 0.116, 'accumulation_steps': 4, 'patch_hidden_dim': 1536, 'patient_hidden_dim': 128, 'patch_dropout': 0.3, 'patient_dropout': 0.2, 'alpha': 0.8, 'mixup': True, 'attention_indicator': True, 'max_node_slices': 15, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.448, 'num_synthetic': 25, 'oversample': 2, 'batch_size': 100, 'lr': 0.00743, 'weight_decay': 0.157, 'accumulation_steps': 5, 'patch_hidden_dim': 2048, 'patient_hidden_dim': 96, 'patch_dropout': 0.4, 'patient_dropout': 0.2, 'alpha': 0.8, 'mixup': False, 'attention_indicator': True, 'max_node_slices': 15, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.425, 'num_synthetic': 25, 'oversample': 1.25, 'batch_size': 150, 'lr': 0.00696, 'weight_decay': 0.0871, 'accumulation_steps': 4, 'patch_hidden_dim': 2048, 'patient_hidden_dim': 128, 'patch_dropout': 0.4, 'patient_dropout': 0.3, 'alpha': 0.8, 'mixup': False, 'attention_indicator': False, 'max_node_slices': 25, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'patient'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.429, 'num_synthetic': 20, 'oversample': 1.5, 'batch_size': 64, 'lr': 0.00662, 'weight_decay': 0.179, 'accumulation_steps': 5, 'patch_hidden_dim': 1536, 'patient_hidden_dim': 96, 'patch_dropout': 0.4, 'patient_dropout': 0.3, 'alpha': 0.7, 'mixup': False, 'attention_indicator': False, 'max_node_slices': 15, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.417, 'num_synthetic': 20, 'oversample': 1.25, 'batch_size': 64, 'lr': 0.00632, 'weight_decay': 0.179, 'accumulation_steps': 3, 'patch_hidden_dim': 512, 'patient_hidden_dim': 128, 'patch_dropout': 0.3, 'patient_dropout': 0.2, 'alpha': 0.8, 'mixup': True, 'attention_indicator': False, 'max_node_slices': 20, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border', 'patient'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.401, 'num_synthetic': 15, 'oversample': 1.25, 'batch_size': 256, 'lr': 0.00749, 'weight_decay': 0.16, 'accumulation_steps': 5, 'patch_hidden_dim': 2560, 'patient_hidden_dim': 96, 'patch_dropout': 0.4, 'patient_dropout': 0.4, 'alpha': 0.8, 'mixup': True, 'attention_indicator': False, 'max_node_slices': 25, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.431, 'num_synthetic': 15, 'oversample': 2, 'batch_size': 150, 'lr': 0.00492, 'weight_decay': 0.199, 'accumulation_steps': 5, 'patch_hidden_dim': 2048, 'patient_hidden_dim': 46, 'patch_dropout': 0.4, 'patient_dropout': 0.3, 'alpha': 0.7, 'mixup': False, 'attention_indicator': True, 'max_node_slices': 25, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border', 'patient'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.419, 'num_synthetic': 25, 'oversample': 1, 'batch_size': 64, 'lr': 0.00628, 'weight_decay': 0.187, 'accumulation_steps': 5, 'patch_hidden_dim': 1536, 'patient_hidden_dim': 64, 'patch_dropout': 0.4, 'patient_dropout': 0.2, 'alpha': 0.7, 'mixup': True, 'attention_indicator': False, 'max_node_slices': 20, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.434, 'num_synthetic': 20, 'oversample': 1.5, 'batch_size': 64, 'lr': 0.00771, 'weight_decay': 0.194, 'accumulation_steps': 5, 'patch_hidden_dim': 2560, 'patient_hidden_dim': 46, 'patch_dropout': 0.4, 'patient_dropout': 0.2, 'alpha': 0.8, 'mixup': False, 'attention_indicator': False, 'max_node_slices': 20, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.409, 'num_synthetic': 25, 'oversample': 1.5, 'batch_size': 150, 'lr': 0.00857, 'weight_decay': 0.172, 'accumulation_steps': 3, 'patch_hidden_dim': 2048, 'patient_hidden_dim': 96, 'patch_dropout': 0.4, 'patient_dropout': 0.3, 'alpha': 0.8, 'mixup': False, 'attention_indicator': False, 'max_node_slices': 25, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'patient'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.407, 'num_synthetic': 30, 'oversample': 1.25, 'batch_size': 128, 'lr': 0.00459, 'weight_decay': 0.176, 'accumulation_steps': 3, 'patch_hidden_dim': 2560, 'patient_hidden_dim': 128, 'patch_dropout': 0.3, 'patient_dropout': 0.2, 'alpha': 0.8, 'mixup': False, 'attention_indicator': True, 'max_node_slices': 25, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'patient'], 'device': device},\n",
    "\n",
    "]\n",
    "#find average for each parameter\n",
    "vae_params_avg = {}\n",
    "for key in vae_params[0].keys():\n",
    "    if key == 'clinical_data_options' or key == 'model_type':\n",
    "        continue\n",
    "    vae_params_avg[key] = sum(d[key] for d in vae_params) / len(vae_params)\n",
    "print(vae_params_avg)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-07T17:38:54.777656700Z",
     "start_time": "2024-11-07T17:38:54.774380200Z"
    }
   },
   "id": "62991e7dd9fe41a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_vae_hyperparams = [{'base': 20, 'latent_size': 20, 'annealing': 1, 'ssim_indicator': 1, 'alpha': 0.4, 'beta': 1, \n",
    "                         'lr':0.000544, 'batch_size': 1536, 'ssim_scalar': 3, 'recon_scale_factor': 4500, \n",
    "                         'weight_decay': 0.0535, 'accumulation_steps': 2}, \n",
    "                        {'base': 24, 'latent_size': 20, 'annealing': 1, 'ssim_indicator': 1, 'alpha': 0.5, 'beta': 1, 'lr': 0.000792, 'batch_size': 1024, 'ssim_scalar': 3, 'recon_scale_factor': 4500, 'weight_decay': 0.0302, 'accumulation_steps': 2},\n",
    "                        {'base': 24, 'latent_size': 16, 'annealing': 1, 'ssim_indicator': 1, 'alpha': 0.4, 'beta': 1, 'lr': 0.000746, 'batch_size': 512, 'ssim_scalar': 3, 'recon_scale_factor': 4500, 'weight_decay': 0.0292, 'accumulation_steps': 2},\n",
    "                        {'base': 24, 'latent_size': 16, 'annealing': 1, 'ssim_indicator': 1, 'alpha': 0.5, 'beta': 1, 'lr': 0.000646, 'batch_size': 1280, 'ssim_scalar': 2, 'recon_scale_factor': 3000, 'weight_decay': 0.0364, 'accumulation_steps': 2},\n",
    "                        {'base': 16, 'latent_size': 24, 'annealing': 1, 'ssim_indicator': 1, 'alpha': 0.5, 'beta': 1, 'lr': 0.000427, 'batch_size': 512, 'ssim_scalar': 3, 'recon_scale_factor': 4000, 'weight_decay': 0.0469, 'accumulation_steps': 3},\n",
    "                         {'base': 20, 'latent_size': 20, 'annealing': 1, 'ssim_indicator': 1, 'alpha': 0.6, 'beta': 1, 'lr': 0.000886, 'batch_size': 512, 'ssim_scalar': 3, 'recon_scale_factor': 3000, 'weight_decay': 0.0135, 'accumulation_steps': 3}]\n",
    "     \n",
    "file_path = r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\fold_data.npy\"\n",
    "first_time = False\n",
    "if first_time:\n",
    "    cohort1 = pd.read_excel(r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1.xlsx\")\n",
    "    IMAGE_DIR = r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1_2D_slices\"\n",
    "    \n",
    "    patient_files_list = [f for f in os.listdir(IMAGE_DIR + '\\mri')] + [f  for f in os.listdir(IMAGE_DIR + '\\mri_aug')]\n",
    "    patient_ids = []\n",
    "    for f in patient_files_list:\n",
    "        patient_id = f[:10]\n",
    "        if patient_id not in patient_ids:\n",
    "            patient_ids.append(patient_id)\n",
    "    \n",
    "    pure_labels = []\n",
    "    for i in range(len(patient_ids)):\n",
    "        N = cohort1[cohort1[('shortpatpseudoid')] == patient_ids[i]]['NodeLabel'].item()\n",
    "        if N == '0':\n",
    "            pure_labels.append(0)\n",
    "        else:\n",
    "            pure_labels.append(1)\n",
    "    \n",
    "    # Initialize KFold for cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)    \n",
    "    # To store the folds\n",
    "    fold_data = []\n",
    "    # Loop over each fold and split\n",
    "    for fold_idx, (train_index, test_index) in enumerate(skf.split(patient_ids, pure_labels)):\n",
    "        # Create train and test sets for current fold\n",
    "        train_ids = [patient_ids[i] for i in train_index]\n",
    "        test_ids = [patient_ids[i] for i in test_index]\n",
    "        train_labels = [pure_labels[i] for i in train_index]\n",
    "        test_labels = [pure_labels[i] for i in test_index]\n",
    "    \n",
    "        # Store the fold's train/test split\n",
    "        fold_data.append([train_ids, test_ids, train_labels, test_labels])\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(fold_data, f)\n",
    "    print(f\"Fold data saved to {file_path}\")\n",
    "    \n",
    "    \n",
    "\n",
    "else:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        fold_data = pickle.load(f)\n",
    "    print(f\"Fold data loaded from {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Run = 25\n",
    "def main(best_hyperparams, i, fold_idx):\n",
    "    global Run, fold_data\n",
    "    IMAGE_DIR = r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1_2D_slices\"\n",
    "    results_path = r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\"\n",
    "\n",
    "\n",
    "    Run += 1\n",
    "    print(\"Run:\", Run)\n",
    "\n",
    "    save_results_path = rf\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_fold_{fold_idx}_run_{Run}.pt\"\n",
    "\n",
    "    # Check if GPU available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "\n",
    "\n",
    "    # settings for reproducibility\n",
    "    torch.manual_seed(int(time.time()))\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    time_start = perf_counter()\n",
    "    \n",
    "    cohort1 = pd.read_excel(r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1.xlsx\")\n",
    "    all_files_list = ['\\mri' + '//' + f for f in os.listdir(IMAGE_DIR + '\\mri')] + ['\\mri_aug' + '//' + f  for f in os.listdir(IMAGE_DIR + '\\mri_aug')]\n",
    "    all_files_list.sort()\n",
    "\n",
    "    # only one scan per patient to avoid repeated scans when saving latent vectors\n",
    "    all_files_list2 = get_single_scan_file_list(all_files_list, IMAGE_DIR, cohort1)\n",
    "    all_files_list2.sort()\n",
    "\n",
    "\n",
    "    epochs = 200\n",
    "\n",
    "\n",
    "    ssim_list, loss_list = [], []\n",
    "\n",
    "    base = best_hyperparams[i]['base']\n",
    "    latent_size = best_hyperparams[i]['latent_size']\n",
    "    annealing = best_hyperparams[i]['annealing']\n",
    "    ssim_indicator = best_hyperparams[i]['ssim_indicator']\n",
    "    alpha = best_hyperparams[i]['alpha']\n",
    "    beta = best_hyperparams[i]['beta']\n",
    "    lr = best_hyperparams[i]['lr']\n",
    "    batch_size = best_hyperparams[i]['batch_size']\n",
    "    ssim_scalar = best_hyperparams[i]['ssim_scalar']\n",
    "    recon_scale_factor = best_hyperparams[i]['recon_scale_factor']\n",
    "    weight_decay = best_hyperparams[i]['weight_decay']\n",
    "    accumulation_steps = best_hyperparams[i]['accumulation_steps']\n",
    "\n",
    "    hyperparams = {'base': base, 'latent_size': latent_size, 'annealing': annealing, 'ssim_indicator': ssim_indicator, 'alpha': alpha, 'beta': beta, 'lr': lr, 'batch_size': batch_size, 'ssim_scalar': ssim_scalar, 'recon_scale_factor': recon_scale_factor, 'weight_decay': weight_decay, 'accumulation_steps': accumulation_steps}\n",
    "    print(\"Using Hyperparams:\", hyperparams)\n",
    "    print(\"latent size:\", latent_size*base)\n",
    "\n",
    "    patient_files_list = [f for f in os.listdir(IMAGE_DIR + '\\mri')] + [f  for f in os.listdir(IMAGE_DIR + '\\mri_aug')]\n",
    "    patient_ids = []\n",
    "    for file in patient_files_list:\n",
    "        if file[0:21] not in patient_ids:\n",
    "            patient_ids.append(file[0:21])\n",
    "\n",
    "    patient_slices_dict, patient_labels_dict, patient_file_names_dict, short_long_axes_dict, mlp_train_ids, test_ids, mlp_train_labels, test_labels, train_images, test_images, train_test_split_dict, mask_sizes_dict = prepare_VAE_MLP_joint_data(first_time_train_test_split=False, cross_val=True, fold_data=fold_data[fold_idx])\n",
    "\n",
    "\n",
    "    train_dataset = LoadImages(main_dir=IMAGE_DIR + '/', files_list=train_images)\n",
    "    test_dataset = LoadImages(main_dir=IMAGE_DIR + '/', files_list=test_images)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    vae_model = VAE_2D(hyperparams)\n",
    "    vae_model = vae_model.to(device)\n",
    "    print('parameter count:', parameter_count(VAE_2D(hyperparams)))\n",
    "    vae_model.apply(weights_init)\n",
    "\n",
    "\n",
    "    vae_model, test_loss, test_ssim, train_loss, train_ssim, VAE_metrics = train_VAE_model(vae_model, epochs, train_loader, test_loader, hyperparams, device, results_path, save_results_path, sample_shape = (12, latent_size*base, 1, 1), train_test_split_dict = train_test_split_dict, wandb_sweep=False, Run=Run, fold_idx=fold_idx)\n",
    "\n",
    "    # if test_ssim > 0.72:\n",
    "    #     plot_results(results_path, save_results_path, 'loss_graph_{}.jpg'.format(Run))\n",
    "    ssim_list.append(test_ssim)\n",
    "    loss_list.append(test_loss)\n",
    "\n",
    "\n",
    "\n",
    "    #idx = loss_list.index(min(loss_list))\n",
    "    print('Hyperparameters:', hyperparams)\n",
    "\n",
    "    # only one scan per patient to avoid repeated scans\n",
    "    if test_ssim > 0.6:\n",
    "        all_files_list2 = get_single_scan_file_list(all_files_list, IMAGE_DIR, cohort1)\n",
    "        all_files_list2.sort()\n",
    "        VAE_metrics, mus = evaluate_VAE(vae_model, test_loss, results_path, batch_size, device, IMAGE_DIR,\n",
    "                                        all_files_list2, feature_length=latent_size*base, Run=Run, wandb_sweep=False, fold_idx=fold_idx)\n",
    "\n",
    "    print(\"Time to train:\", perf_counter() - time_start)\n",
    "    print(f\"Cooling down for 5 mins...\")\n",
    "    time.sleep(300)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f220dd889552861"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To run VAE cross-val\n",
    "# for fold_idx in range(5):\n",
    "#     for i in range(6):\n",
    "#         main(best_vae_hyperparams, i, fold_idx)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40f8b94bf0a9aa56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(project=\"Cross_validation_VAE_MLP4\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "best_ten_MLP_params = [\n",
    "    {'num_epochs': 200, 'threshold': 0.5, 'num_synthetic': 25, 'oversample': 1.5, 'batch_size': 128, 'lr': 0.006, 'weight_decay': 0.14, 'accumulation_steps': 4, 'patch_hidden_dim': 2048, 'patient_hidden_dim': 128, 'patch_dropout': 0.35, 'patient_dropout': 0.3, 'alpha': 0.8, 'mixup': False, 'attention_indicator': True, 'max_node_slices': 15, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border', 'patient'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.509, 'num_synthetic': 25, 'oversample': 2, 'batch_size': 100, 'lr': 0.00621, 'weight_decay': 0.116, 'accumulation_steps': 4, 'patch_hidden_dim': 1536, 'patient_hidden_dim': 128, 'patch_dropout': 0.3, 'patient_dropout': 0.2, 'alpha': 0.8, 'mixup': True, 'attention_indicator': True, 'max_node_slices': 15, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border'], 'device': device}, \n",
    "    {'num_epochs': 200, 'threshold': 0.448, 'num_synthetic': 25, 'oversample': 2, 'batch_size': 100, 'lr': 0.00743, 'weight_decay': 0.157, 'accumulation_steps': 5, 'patch_hidden_dim': 2048, 'patient_hidden_dim': 96, 'patch_dropout': 0.4, 'patient_dropout': 0.2, 'alpha': 0.8, 'mixup': False, 'attention_indicator': True, 'max_node_slices': 15, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.425, 'num_synthetic': 25, 'oversample': 1.25, 'batch_size': 150, 'lr': 0.00696, 'weight_decay': 0.0871, 'accumulation_steps': 4, 'patch_hidden_dim': 2048, 'patient_hidden_dim': 128, 'patch_dropout': 0.4, 'patient_dropout': 0.3, 'alpha': 0.8, 'mixup': False, 'attention_indicator': False, 'max_node_slices': 25, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'patient'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.429, 'num_synthetic': 20, 'oversample': 1.5, 'batch_size': 64, 'lr': 0.00662, 'weight_decay': 0.179, 'accumulation_steps': 5, 'patch_hidden_dim': 1536, 'patient_hidden_dim': 96, 'patch_dropout': 0.4, 'patient_dropout': 0.3, 'alpha': 0.7, 'mixup': False, 'attention_indicator': False, 'max_node_slices': 15, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.417, 'num_synthetic': 20, 'oversample': 1.25, 'batch_size': 64, 'lr': 0.00632, 'weight_decay': 0.179, 'accumulation_steps': 3, 'patch_hidden_dim': 512, 'patient_hidden_dim': 128, 'patch_dropout': 0.3, 'patient_dropout': 0.2, 'alpha': 0.8, 'mixup': True, 'attention_indicator': False, 'max_node_slices': 20, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border', 'patient'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.401, 'num_synthetic': 15, 'oversample': 1.25, 'batch_size': 256, 'lr': 0.00749, 'weight_decay': 0.16, 'accumulation_steps': 5, 'patch_hidden_dim': 2560, 'patient_hidden_dim': 96, 'patch_dropout': 0.4, 'patient_dropout': 0.4, 'alpha': 0.8, 'mixup': True, 'attention_indicator': False, 'max_node_slices': 25, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.431, 'num_synthetic': 15, 'oversample': 2, 'batch_size': 150, 'lr': 0.00492, 'weight_decay': 0.199, 'accumulation_steps': 5, 'patch_hidden_dim': 2048, 'patient_hidden_dim': 46, 'patch_dropout': 0.4, 'patient_dropout': 0.3, 'alpha': 0.7, 'mixup': False, 'attention_indicator': True, 'max_node_slices': 25, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border', 'patient'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.419, 'num_synthetic': 25, 'oversample': 1, 'batch_size': 64, 'lr': 0.00628, 'weight_decay': 0.187, 'accumulation_steps': 5, 'patch_hidden_dim': 1536, 'patient_hidden_dim': 64, 'patch_dropout': 0.4, 'patient_dropout': 0.2, 'alpha': 0.7, 'mixup': True, 'attention_indicator': False, 'max_node_slices': 20, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.434, 'num_synthetic': 20, 'oversample': 1.5, 'batch_size': 64, 'lr': 0.00771, 'weight_decay': 0.194, 'accumulation_steps': 5, 'patch_hidden_dim': 2560, 'patient_hidden_dim': 46, 'patch_dropout': 0.4, 'patient_dropout': 0.2, 'alpha': 0.8, 'mixup': False, 'attention_indicator': False, 'max_node_slices': 20, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'border'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.409, 'num_synthetic': 25, 'oversample': 1.5, 'batch_size': 150, 'lr': 0.00857, 'weight_decay': 0.172, 'accumulation_steps': 3, 'patch_hidden_dim': 2048, 'patient_hidden_dim': 96, 'patch_dropout': 0.4, 'patient_dropout': 0.3, 'alpha': 0.8, 'mixup': False, 'attention_indicator': False, 'max_node_slices': 25, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'patient'], 'device': device},\n",
    "    {'num_epochs': 200, 'threshold': 0.407, 'num_synthetic': 30, 'oversample': 1.25, 'batch_size': 128, 'lr': 0.00459, 'weight_decay': 0.176, 'accumulation_steps': 3, 'patch_hidden_dim': 2560, 'patient_hidden_dim': 128, 'patch_dropout': 0.3, 'patient_dropout': 0.2, 'alpha': 0.8, 'mixup': False, 'attention_indicator': True, 'max_node_slices': 25, 'model_type': 'MLP_MIL_model2', 'clinical_data_options': ['T_stage', 'size', 'patient'], 'device': device},\n",
    "\n",
    "]\n",
    "\n",
    "latent_vector_paths = [\n",
    "                       r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_MLP_cross_validation\\latent_vectors_fold_0_run_5.npy\",\n",
    "                       r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_MLP_cross_validation\\latent_vectors_fold_1_run_11.npy\",\n",
    "                       r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_MLP_cross_validation\\latent_vectors_fold_2_run_14.npy\",\n",
    "                       r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_MLP_cross_validation\\latent_vectors_fold_3_run_23.npy\",\n",
    "                       r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_MLP_cross_validation\\latent_vectors_fold_4_run_26.npy\"\n",
    "                        ]   \n",
    "\n",
    "VAE_params_paths = [\n",
    "                    r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_MLP_cross_validation\\VAE_fold_0_run_5_ssim_0.727695107460022.pt\",\n",
    "                    r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_MLP_cross_validation\\VAE_fold_1_run_11_ssim_0.7619633674621582.pt\",\n",
    "                    r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_MLP_cross_validation\\VAE_fold_2_run_14_ssim_0.7635628581047058.pt\",\n",
    "                    r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_MLP_cross_validation\\VAE_fold_3_run_23_ssim_0.77414470911026.pt\",\n",
    "                    r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_MLP_cross_validation\\VAE_fold_4_run_26_ssim_0.7822268009185791.pt\"\n",
    "                    ]\n",
    "\n",
    "                   \n",
    "Run = 0\n",
    "best_test_preds, best_test_probs= [], []\n",
    "def main(best_hyperparams, i, fold_idx, latent_vector_paths, VAE_params_paths):\n",
    "    global Run, best_test_preds, best_test_probs\n",
    "    if fold_idx < 3:\n",
    "        best_score = {'TP': 7, 'FP': 7, 'Train_Sensitivity': 0.6}\n",
    "    if fold_idx >= 3:\n",
    "        best_score = {'TP': 6, 'FP': 7, 'Train_Sensitivity': 0.6}\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    Run += 1\n",
    "    print('Run:', Run)\n",
    "    num_logged = 0\n",
    "    \n",
    "    run = wandb.init(project=\"Cross_validation_VAE_MLP4\", name=f\"fold_{fold_idx}_run_{Run}\", reinit=True)\n",
    "    wandb.config.update(best_hyperparams[i])\n",
    "    \n",
    "    # Hyperparams    \n",
    "    num_epochs = 200\n",
    "    \n",
    "    num_synthetic = best_hyperparams[i]['num_synthetic']\n",
    "    oversample = best_hyperparams[i]['oversample']\n",
    "    max_node_slices = best_hyperparams[i]['max_node_slices']\n",
    "    threshold = best_hyperparams[i]['threshold']\n",
    "    batch_size = best_hyperparams[i]['batch_size']\n",
    "    lr = best_hyperparams[i]['lr']\n",
    "    weight_decay = best_hyperparams[i]['weight_decay']\n",
    "    accumulation_steps = best_hyperparams[i]['accumulation_steps']\n",
    "    patch_hidden_dim = best_hyperparams[i]['patch_hidden_dim']\n",
    "    patient_hidden_dim = best_hyperparams[i]['patient_hidden_dim']\n",
    "    patch_dropout = best_hyperparams[i]['patch_dropout']\n",
    "    patient_dropout = best_hyperparams[i]['patient_dropout']\n",
    "    alpha = best_hyperparams[i]['alpha']\n",
    "    mixup = best_hyperparams[i]['mixup']\n",
    "    attention_indicator = best_hyperparams[i]['attention_indicator']\n",
    "    model_type = best_hyperparams[i]['model_type']\n",
    "    clinical_data_options = best_hyperparams[i]['clinical_data_options']\n",
    "    \n",
    "    clinical_length = 0\n",
    "    if \"size\" in clinical_data_options:\n",
    "        clinical_length += 3\n",
    "    if \"border\" in clinical_data_options:\n",
    "        clinical_length += 2\n",
    "\n",
    "    hyperparams = {'num_epochs': num_epochs, 'threshold': threshold, 'num_synthetic': num_synthetic, 'oversample': oversample,\n",
    "                   'batch_size': batch_size, 'lr': lr, 'weight_decay': weight_decay, 'accumulation_steps': accumulation_steps,\n",
    "                   'patch_hidden_dim': patch_hidden_dim, 'patient_hidden_dim': patient_hidden_dim,\n",
    "                   'patch_dropout': patch_dropout, 'patient_dropout': patient_dropout, 'alpha': alpha, 'mixup': mixup,\n",
    "                   'attention_indicator': attention_indicator, 'max_node_slices': max_node_slices, 'model_type': model_type,\n",
    "                   'clinical_data_options': clinical_data_options, 'device': device}\n",
    "\n",
    "    print(hyperparams)\n",
    "    print('Device:', device)\n",
    "\n",
    "    time_start = perf_counter()\n",
    "\n",
    "    results_path = r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_MLP_cross_validation\\MLP_Results\"\n",
    "    save_results_path = rf\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE_MLP_cross_validation\\MLP_Results\\MLP_Fold_{fold_idx}_Run_{Run}.pt\"\n",
    "\n",
    "    # Load the dataset\n",
    "    IMAGE_DIR = r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1_2D_slices\"\n",
    "    cohort1 = pd.read_excel(r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1.xlsx\")\n",
    "    latent_vectors = np.load(latent_vector_paths[fold_idx])\n",
    "\n",
    "    all_files_list = ['\\mri' + '//' + f for f in os.listdir(IMAGE_DIR + '\\mri')] + ['\\mri_aug' + '//' + f  for f in os.listdir(IMAGE_DIR + '\\mri_aug')]\n",
    "    all_files_list.sort()\n",
    "    all_files_list = get_single_scan_file_list(all_files_list, IMAGE_DIR, cohort1)\n",
    "\n",
    "    VAE_params_path = VAE_params_paths[fold_idx]\n",
    "    checkpoint = torch.load(VAE_params_path)\n",
    "    train_test_split_dict = checkpoint['train_test_split']\n",
    "    train_ids = train_test_split_dict['train']\n",
    "    test_ids = train_test_split_dict['test']\n",
    "    patient_slices_dict, patient_labels_dict, patient_file_names_dict, short_long_axes_dict, mlp_train_ids, test_ids, mlp_train_labels, test_labels, train_images, test_images, train_test_split_dict, mask_sizes = prepare_VAE_MLP_joint_data(first_time_train_test_split=False, train_ids=train_ids, test_ids=test_ids, num_synthetic=num_synthetic, oversample_ratio=oversample)\n",
    "    \n",
    "    #train_dataset = torch.load(r'C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\MLP_Results\\{}.pth'.format(dataset_version))\n",
    "    train_dataset = Load_Latent_Vectors(patient_slices_dict, latent_vectors, patient_labels_dict, mlp_train_ids, cohort1, all_files_list, short_long_axes_dict, mask_sizes, clinical_data_options, max_nodes=max_node_slices)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    #test_dataset = torch.load(r'C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\MLP_Results\\test_dataset.pth')\n",
    "    test_dataset = Load_Latent_Vectors(patient_slices_dict, latent_vectors, patient_labels_dict, test_ids, cohort1, all_files_list, short_long_axes_dict, mask_sizes, clinical_data_options, max_nodes=max_node_slices)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "    \n",
    "    vae_hyperparams = checkpoint['hyperparams']\n",
    "    base = vae_hyperparams['base']\n",
    "    latent_size = vae_hyperparams['latent_size']\n",
    "    vae_features_length = base*latent_size\n",
    "    # Instantiate the model\n",
    "    if model_type == 'MLP_MIL_model_simple':\n",
    "        model = MLP_MIL_model_simple(patch_input_dim=vae_features_length+clinical_length, hyperparams=hyperparams)\n",
    "    if model_type == 'MLP_MIL_model2':\n",
    "        model = MLP_MIL_model2(patch_input_dim=vae_features_length+clinical_length, hyperparams=hyperparams)\n",
    "\n",
    "    model.apply(weights_init)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', factor=0.5, patience=40,\n",
    "                                                              verbose=True, threshold=0.001, threshold_mode='abs')\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    train_AUCs, test_AUCs = [], []\n",
    "    train_sensitivitys, test_sensitivitys = [], []\n",
    "    batches_mixed = 0\n",
    "    early_stopping = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        all_train_labels = []\n",
    "        all_train_preds = []\n",
    "        all_train_probs = []\n",
    "        steps = 0\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "\n",
    "        for features, label, clinical_data, number_of_nodes in train_dataloader:\n",
    "            if mixup:\n",
    "                random_int = np.random.randint(1, 6) # random int between 1 and 3\n",
    "            else:\n",
    "                random_int = 0\n",
    "            if random_int == 1: # 1/6 chance\n",
    "                #print('Mixup')\n",
    "                batches_mixed += 1\n",
    "\n",
    "                features_without_clinical = features[:, :, :-clinical_length]\n",
    "                features_mixed = mixup_batch(features_without_clinical) # mixup 50% of batch\n",
    "                if clinical_length > 0:\n",
    "                    features = torch.cat((features_mixed, features[:, :, -clinical_length:]), dim=2) # add clinical data back\n",
    "            if random_int == 2: # 1/6 chance\n",
    "                #print('Noise')\n",
    "                batches_mixed += 1\n",
    "                features_without_clinical = features[:, :, :-clinical_length]\n",
    "                features_noise = process_batch_with_noise(features_without_clinical) # add noise to 50% of batch\n",
    "                if clinical_length > 0:\n",
    "                    features = torch.cat((features_noise, features[:, :, -clinical_length:]), dim=2) # add clinical data back\n",
    "\n",
    "            features, label = features.to(device), label.to(device)\n",
    "            clinical_data, number_of_nodes = clinical_data.to(device), number_of_nodes.to(device)\n",
    "            steps += 1\n",
    "            # Forward pass\n",
    "            #output = model(features.squeeze(0))  # Remove batch dimension\n",
    "            output, max_vals, attentions, classifications = model(features, clinical_data, number_of_nodes, label)\n",
    "            output = output.squeeze(1)\n",
    "\n",
    "            # binary threshold classifications\n",
    "            # classifications = torch.where(classifications > 0.5, torch.tensor([1.]).to(device), torch.tensor([0.]).to(device))\n",
    "            # print('Classifications:', classifications)\n",
    "\n",
    "\n",
    "            #print(torch.mean(label.float()))\n",
    "            # if label == 1:\n",
    "            #     weight = torch.tensor([2.0]).to(device)\n",
    "            # if label == 0:\n",
    "            #     weight = torch.tensor([1.0]).to(device)\n",
    "\n",
    "            loss = criterion(output, label.float()) #*weight\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # # Backward pass and optimization\n",
    "            # optimiser.zero_grad()\n",
    "            # loss.backward()\n",
    "            # optimiser.step()\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            if steps % accumulation_steps == 1:\n",
    "                optimiser.step()\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "            # Apply threshold to determine predicted class\n",
    "            #predicted_probs = F.softmax(output, dim=1)[:, 1]  # Probability of class 1 (positive)\n",
    "            #predicted_probs = torch.sigmoid(output)\n",
    "\n",
    "            predicted_probs = output\n",
    "            classifications_class = (classifications >= threshold).long()\n",
    "            #predicted_probs = 0.6*max_vals + 0.35*classifications_class + 0.05*attentions\n",
    "            predicted_class = (predicted_probs >= threshold).long()\n",
    "\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_train_labels.extend(label.cpu().numpy())\n",
    "            all_train_preds.extend(predicted_class.cpu().numpy())\n",
    "            all_train_probs.extend(predicted_probs.tolist())\n",
    "            # random_int = np.random.randint(1, 20)\n",
    "            # if random_int == 1:\n",
    "            #     rdn_idx  = np.random.randint(0, len(features))\n",
    "            #     print(rdn_idx, 'label (train)', label[rdn_idx].item(), 'output', output[rdn_idx].item(), 'predicted class', predicted_class[rdn_idx].item(), 'max', max_vals[rdn_idx].item(), 'attention', attentions[rdn_idx].item(), 'classification', classifications[rdn_idx].item(), 'class binary', classifications_class[rdn_idx].item(), 'number of nodes', number_of_nodes[rdn_idx].item()) # 'reweighted prediction', predicted_probs[rdn_idx].item())\n",
    "\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "        lr_scheduler.step(train_loss/len(train_dataloader))\n",
    "        if epoch % 5 == 0 or epoch + 20 > num_epochs-1:\n",
    "            print('Learning rate:', optimiser.param_groups[0]['lr'])\n",
    "        train_losses.append(train_loss/len(train_dataloader))\n",
    "        train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
    "        train_auc = roc_auc_score(all_train_labels, all_train_preds)\n",
    "        train_AUCs.append(train_auc)\n",
    "        train_bal_accuracy = balanced_accuracy_score(all_train_labels, all_train_preds)\n",
    "        train_confusion_matrix = confusion_matrix(all_train_labels, all_train_preds)\n",
    "        tn, fp, fn, tp = confusion_matrix(all_train_labels, all_train_preds).ravel()\n",
    "        # Compute sensitivity (recall) and specificity\n",
    "        train_sensitivity = tp / (tp + fn)\n",
    "        train_specificity = tn / (tn + fp)\n",
    "        train_sensitivitys.append(train_sensitivity)\n",
    "\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train: Loss: {train_loss/len(train_dataloader):.4f}, Accuracy: {train_accuracy:.4f}, Balanced Accuracy: {train_bal_accuracy:.4f}, AUC: {train_auc:.4f}, Sensitivity: {train_sensitivity:.4f}, Specificity: {train_specificity:.4f}')\n",
    "        print(f'Train Confusion Matrix:')\n",
    "        print(train_confusion_matrix)\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        all_test_labels = []\n",
    "        all_test_preds = []\n",
    "        all_test_probs = []\n",
    "        with torch.no_grad():\n",
    "            for features, label, clinical_data, number_of_nodes in test_dataloader:\n",
    "                features, label = features.to(device), label.to(device)\n",
    "                clinical_data, number_of_nodes = clinical_data.to(device), number_of_nodes.to(device)\n",
    "                #output = model(features.squeeze(0))  # Remove batch dimension\n",
    "                output, max_vals, attentions, classifications = model(features, clinical_data, number_of_nodes, label)\n",
    "                output = output.squeeze(1)\n",
    "\n",
    "                #output = output.squeeze(0)\n",
    "                loss = criterion(output, label.float())\n",
    "                test_loss += loss.item()\n",
    "\n",
    "\n",
    "                # Store predictions and labels\n",
    "                #predicted_probs = F.softmax(output, dim=1)[:, 1]  # Probability of class 1 (positive)\n",
    "                #predicted_probs = torch.sigmoid(output)\n",
    "                predicted_probs = output\n",
    "                classifications_class = (classifications >= threshold).long()\n",
    "                #predicted_probs = 0.6*max_vals + 0.35*classifications_class + 0.05*attentions\n",
    "                predicted_class = (predicted_probs >= threshold).type(torch.long)\n",
    "                all_test_labels.extend(label.cpu().numpy())\n",
    "                all_test_preds.extend(predicted_class.cpu().numpy())\n",
    "                all_test_probs.extend(predicted_probs.cpu().numpy())\n",
    "\n",
    "\n",
    "        test_losses.append(test_loss/len(test_dataloader))\n",
    "        test_accuracy = accuracy_score(all_test_labels, all_test_preds)\n",
    "        test_auc = roc_auc_score(all_test_labels, all_test_preds)\n",
    "        test_AUCs.append(test_auc)\n",
    "        test_bal_accuracy = balanced_accuracy_score(all_test_labels, all_test_preds)\n",
    "        test_confusion_matrix = confusion_matrix(all_test_labels, all_test_preds)\n",
    "        tn, fp, fn, tp = confusion_matrix(all_test_labels, all_test_preds).ravel()\n",
    "        # Compute sensitivity (recall) and specificity\n",
    "        test_sensitivity = tp / (tp + fn)\n",
    "        test_specificity = tn / (tn + fp)\n",
    "        test_metric = (2*test_sensitivity + test_specificity)/3\n",
    "        test_sensitivitys.append(test_sensitivity)\n",
    "        #if epoch % 5 == 0 or epoch + 20 > num_epochs-1:\n",
    "        print(f'Test: Loss: {test_loss/len(test_dataloader):.4f}, Accuracy: {test_accuracy:.4f}, Balanced Accuracy: {test_bal_accuracy:.4f}, AUC: {test_auc:.4f}, Sensitivity: {test_sensitivity:.4f}, Specificity: {test_specificity:.4f}, Metric:, {test_metric:.4f}')\n",
    "        print('Test Confusion Matrix:')\n",
    "        print(test_confusion_matrix)\n",
    "        # Wait for GPU to cool down for 10 seconds\n",
    "        time.sleep(10)\n",
    "\n",
    "        if epoch == 0:\n",
    "            test_labels = np.array(all_test_labels)\n",
    "\n",
    "        if tp >= 6 and fp <= 8:\n",
    "            # error analysis\n",
    "            best_test_preds.append(all_test_preds)\n",
    "            best_test_probs.append(all_test_probs)\n",
    "            print('number of preds logged:', len(best_test_probs))\n",
    "            error_analysis(np.array(best_test_probs), test_labels, results_path, threshold, fold_idx)\n",
    "            num_logged += 1\n",
    "\n",
    "            if tp > best_score['TP'] or (tp >= best_score['TP'] and fp < best_score['FP']) or (tp >= best_score['TP'] and fp <= best_score['FP'] and train_sensitivity > best_score['Train_Sensitivity']):\n",
    "                best_score['TP'] = tp\n",
    "                best_score['FP'] = fp\n",
    "                best_score['Train_Sensitivity'] = train_sensitivity\n",
    "                print('Saving model with TP:', tp, 'and FP:', fp, 'at epoch:', epoch)\n",
    "                training_results = {'train_losses': train_losses, 'test_losses': test_losses, 'train_AUCs': train_AUCs, \n",
    "                                    'test_AUCs': test_AUCs, 'train_sensitivitys': train_sensitivitys, 'test_sensitivitys': test_sensitivitys, 'all test labels': all_test_labels, 'all test probs': all_test_probs}\n",
    "                torch.save({\"state_dict\": model.state_dict(), \"training_results\": training_results,\n",
    "                            \"hyperparams\": hyperparams, \"train_test_split\": train_test_split_dict}, save_results_path)\n",
    "                calibration_curve_and_distribution(all_train_labels, all_train_probs, 'Train', results_path, f'saved_result_fold_{fold_idx}_run_{Run}', save=True)\n",
    "                calibration_curve_and_distribution(all_test_labels, all_test_probs, 'Test', results_path, f'saved_result_fold_{fold_idx}_run_{Run}', save=True)\n",
    "\n",
    "                # plot results at this stage (updating until the best run)\n",
    "                plot_MLP_results(training_results, hyperparams, results_path=results_path, filename=f'MLP_training_metrics_{fold_idx}_run_{Run}.png')\n",
    "\n",
    "\n",
    "\n",
    "        if epoch == num_epochs-1:\n",
    "            calibration_curve_and_distribution(all_train_labels, all_train_probs, 'Train', results_path, Run)\n",
    "            calibration_curve_and_distribution(all_test_labels, all_test_probs, 'Test', results_path, Run)\n",
    "\n",
    "        print('Max Test AUC:', np.max(test_AUCs))\n",
    "        # log to wandb\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"Test Loss\": test_loss/len(test_dataloader),\n",
    "                \"Test Metric\": test_metric,\n",
    "                \"Test Accuracy\": test_accuracy,\n",
    "                \"Test AUC\": test_auc,\n",
    "                \"Test Sensitivity\": test_sensitivity,\n",
    "                \"Test Specificity\": test_specificity,\n",
    "                \"Test TP\": tp,\n",
    "                \"Test FP\": fp,\n",
    "                \"Train Loss\": train_loss/len(train_dataloader),\n",
    "                \"Train Accuracy\": train_accuracy,\n",
    "                \"Train AUC\": train_auc,\n",
    "                \"Train Sensitivity\": train_sensitivity,\n",
    "                \"Train Specificity\": train_specificity,\n",
    "                \"Max Test AUC\": np.max(test_AUCs),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # # Early stopping\n",
    "        # if epoch > 75 and test_auc < 0.7:\n",
    "        #     early_stopping+=1\n",
    "        #     if early_stopping > 25 and test_auc < 0.6 and num_logged == 0:\n",
    "        #         print('Early stopping')\n",
    "        #         break\n",
    "        #     if early_stopping > 50 and test_auc < 0.65 and num_logged == 0:\n",
    "        #         print('Early stopping')\n",
    "        #         break\n",
    "        # \n",
    "        #     if early_stopping > 75 and test_auc < 0.7 and num_logged <= 1:\n",
    "        #         print('Early stopping')\n",
    "        #         break\n",
    "\n",
    "\n",
    "    # save test preds and probs\n",
    "    np.save(results_path + '//best_test_preds.npy', np.array(best_test_preds))\n",
    "    np.save(results_path + '//best_test_probs.npy', np.array(best_test_probs))\n",
    "\n",
    "    print('Batches mixed:', batches_mixed, 'out of', len(train_dataloader)*num_epochs, 'percentage:', batches_mixed/(len(train_dataloader)*num_epochs))\n",
    "    print(hyperparams)\n",
    "    print('Time taken:', perf_counter() - time_start)\n",
    "    # Wait for GPU to cool down after each model run\n",
    "    print(f\"Cooling down for 5 mins...\")\n",
    "    time.sleep(60*5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80b8462c14fe5d16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_start = perf_counter()\n",
    "folds = [1]\n",
    "\n",
    "for i in range(12):\n",
    "    for fold_idx in folds:\n",
    "        print('starting fold:', fold_idx, 'run:', i)\n",
    "        main(best_ten_MLP_params, i, fold_idx, latent_vector_paths, VAE_params_paths)\n",
    "print('Total time taken:', perf_counter() - time_start)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cacf0da86d191d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
