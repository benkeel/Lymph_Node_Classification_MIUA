{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter sweep for CNN-MLP model (replacing VAE with CNN backbone)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e098befe9b7b05b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-05T21:18:52.647747200Z",
     "start_time": "2024-11-05T21:18:47.843404Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "from monai.networks.nets import DenseNet121\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "import torchvision.models as models\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils.datasets import Load_CNN_Images, prepare_VAE_MLP_joint_data\n",
    "from models.MLP_model import MLP_MIL_model_simple, MLP_MIL_model2\n",
    "from utils.utility_code import get_single_scan_file_list, get_class_distribution, weights_init, plot_MLP_results, error_analysis\n",
    "from utils.train_and_test_functions import mixup_patient_data, mixup_batch, process_batch_with_noise, calibration_curve_and_distribution\n",
    "\n",
    "import wandb\n",
    "from monai.networks.nets import DenseNet #, HighResNet, EfficientNet, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Best so far: {'num_epochs': 200, 'threshold': 0.4049798489191535, 'num_synthetic': 30, 'oversample': 1.5, 'batch_size': 64, 'lr': 0.004089429701418951, 'weight_decay': 0.08306021271710541, 'accumulation_steps': 3, 'patch_hidden_dim': 2048, 'max_node_slices': 15, 'model_type': 'MLP_MIL_model2'}\n",
    "\n",
    "# Define sweep configuration\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"name\": \"sweep3\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"Max Test AUC\"},\n",
    "    \"parameters\": {\n",
    "        #\"dataset_version\": {\"values\": dataset_version},\n",
    "        \"num_synthetic\": {\"values\": [10, 20, 25, 30]},\n",
    "        \"oversample\": {\"values\": [1, 1.25, 1.5]},\n",
    "        \"max_node_slices\": {\"values\": [15, 20, 25, 30]},\n",
    "        \"threshold\": {\"max\": 0.55, \"min\": 0.4},\n",
    "        \"batch_size\": {\"values\": [64, 100, 128, 150]},\n",
    "        \"lr\": {\"max\": 0.01, \"min\": 0.0001},\n",
    "        \"weight_decay\": {\"max\": 0.2, \"min\": 0.04},\n",
    "        \"accumulation_steps\": {\"values\": [2, 3, 4, 5]},\n",
    "        \"patch_hidden_dim\": {\"values\": [128, 256, 512, 1024, 1536, 2048, 2560]},\n",
    "        \"patient_hidden_dim\": {\"values\": [16, 24, 32, 36, 46, 64, 96, 128]},\n",
    "        \"patch_dropout\": {\"values\": [0.2, 0.3, 0.4]},\n",
    "        \"patient_dropout\": {\"values\": [0.2, 0.3, 0.4]},\n",
    "        \"alpha\": {\"values\": [0.2, 0.6, 0.8]}, # \"alpha*max_vals + (0.9-alpha)*classifications + 0.1*attentions\n",
    "        \"attention_indicator\": {\"values\": [True, False]},\n",
    "        \"model_type\": {\"values\": [\"MLP_MIL_model2\"]}, #\"MLP_MIL_model_simple\",\n",
    "        \"clinical_data_options\": {\"values\": [[\"T_stage\", \"size\", \"border\", \"patient\"], [\"T_stage\", \"size\", \"border\"], [\"T_stage\", \"size\", \"patient\"], [\"T_stage\", \"border\", \"patient\"]]},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialize sweep by passing in config.\n",
    "# Provide a name of the project.\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"CNN-MLP-bayesian-sweep2\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "952f052034708bcc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class InstanceCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InstanceCNN, self).__init__()\n",
    "\n",
    "        # model = models.resnet18(pretrained=True)\n",
    "        # # Modify first convolutional layer to accept single channel input\n",
    "        # # Original in_channels for ResNet-18 is 3\n",
    "        # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        #self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "        model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=1, pretrained=True)\n",
    "        self.features = nn.Sequential(*list(model.children())[:-1],\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.AdaptiveAvgPool2d(output_size=1))\n",
    "\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T21:20:09.259011200Z",
     "start_time": "2024-11-05T21:20:09.254968400Z"
    }
   },
   "id": "a952140632d15e00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b957f4d754cda38"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "Run = 0\n",
    "best_test_preds, best_test_probs= [], []\n",
    "def main():\n",
    "    global Run, best_test_preds, best_test_probs\n",
    "    best_score = {'TP': 10, 'FP':10, 'Train_Sensitivity': 0.6}\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    run = wandb.init()\n",
    "    Run += 1\n",
    "    print('Run:', Run)\n",
    "    num_logged = 0\n",
    "    # Hyperparams    \n",
    "    num_epochs = 200\n",
    "    #dataset_version = wandb.config.dataset_version\n",
    "    num_synthetic = wandb.config.num_synthetic\n",
    "    oversample = wandb.config.oversample\n",
    "    max_node_slices = wandb.config.max_node_slices\n",
    "    threshold = wandb.config.threshold\n",
    "    batch_size = wandb.config.batch_size\n",
    "    lr = wandb.config.lr\n",
    "    weight_decay = wandb.config.weight_decay\n",
    "    accumulation_steps = wandb.config.accumulation_steps\n",
    "    patch_hidden_dim = wandb.config.patch_hidden_dim\n",
    "    patient_hidden_dim = wandb.config.patient_hidden_dim\n",
    "    patch_dropout = wandb.config.patch_dropout\n",
    "    patient_dropout = wandb.config.patient_dropout\n",
    "    model_type = wandb.config.model_type\n",
    "    attention_indicator = wandb.config.attention_indicator\n",
    "    alpha = wandb.config.alpha\n",
    "    clinical_data_options = wandb.config.clinical_data_options\n",
    "    clinical_length = 0\n",
    "    if \"size\" in clinical_data_options:\n",
    "        clinical_length += 3\n",
    "    if \"border\" in clinical_data_options:\n",
    "        clinical_length += 2\n",
    "\n",
    "    hyperparams = {'num_epochs': num_epochs, 'threshold': threshold, 'num_synthetic': num_synthetic, 'oversample': oversample,\n",
    "                   'batch_size': batch_size, 'lr': lr, 'weight_decay': weight_decay, 'accumulation_steps': accumulation_steps,\n",
    "                   'patch_hidden_dim': patch_hidden_dim, 'patient_hidden_dim': patient_hidden_dim,\n",
    "                   'patch_dropout': patch_dropout, 'patient_dropout': patient_dropout, 'alpha': alpha,\n",
    "                   'attention_indicator': attention_indicator, 'max_node_slices': max_node_slices, 'model_type': model_type,\n",
    "                   'clinical_data_options': clinical_data_options, 'device': device}\n",
    "\n",
    "    print(hyperparams)\n",
    "    print('Device:', device)\n",
    "\n",
    "    time_start = perf_counter()\n",
    "\n",
    "    results_path = r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\CNN_MLP_Results\"\n",
    "    save_results_path = rf\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\CNN_MLP_Results\\MLP_{Run}.pt\"\n",
    "    # Load the dataset\n",
    "    IMAGE_DIR = r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1_2D_slices\"\n",
    "    cohort1 = pd.read_excel(r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1.xlsx\")\n",
    "    latent_vectors = np.load(r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE2_results\\latent_vectors_36.npy\")\n",
    "\n",
    "\n",
    "\n",
    "    VAE_params_path = r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE2_results\\VAE_36.pt\"\n",
    "    checkpoint = torch.load(VAE_params_path)\n",
    "    train_test_split_dict = checkpoint['train_test_split']\n",
    "    train_ids = train_test_split_dict['train']\n",
    "    test_ids = train_test_split_dict['test']\n",
    "    patient_slices_dict, patient_labels_dict, patient_file_names_dict, short_long_axes_dict, mlp_train_ids, test_ids, mlp_train_labels, test_labels, train_images, test_images, train_test_split_dict, mask_sizes = prepare_VAE_MLP_joint_data(first_time_train_test_split=False, train_ids=train_ids, test_ids=test_ids, num_synthetic=num_synthetic, oversample_ratio=oversample)\n",
    "\n",
    "    all_files_list = ['\\mri' + '//' + f for f in os.listdir(IMAGE_DIR + '\\mri')] + ['\\mri_aug' + '//' + f  for f in os.listdir(IMAGE_DIR + '\\mri_aug')]\n",
    "    all_files_list.sort()\n",
    "    all_files_list = get_single_scan_file_list(all_files_list, IMAGE_DIR, cohort1)\n",
    "\n",
    "    patient_file_names_dict = {}\n",
    "    for patient in patient_slices_dict.keys():\n",
    "        for idx in patient_slices_dict[patient]:\n",
    "            if patient in patient_file_names_dict.keys():\n",
    "                patient_file_names_dict[patient].append(all_files_list[idx])\n",
    "            else:\n",
    "                patient_file_names_dict[patient] = [all_files_list[idx]]\n",
    "\n",
    "    train_dataset = Load_CNN_Images(patient_file_names_dict, patient_labels_dict, mlp_train_ids, cohort1, all_files_list, short_long_axes_dict, mask_sizes, clinical_data_options, max_nodes=max_node_slices)\n",
    "    \n",
    "    test_dataset = Load_CNN_Images(patient_file_names_dict, patient_labels_dict, test_ids, cohort1, all_files_list, short_long_axes_dict, mask_sizes, clinical_data_options, max_nodes=max_node_slices)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "    # Initialise CNN model, loss function, and optimizer\n",
    "    backbone = InstanceCNN()\n",
    "    patch_input_dim = 1024\n",
    "\n",
    "    # Instantiate the model\n",
    "    if model_type == 'MLP_MIL_model_simple':\n",
    "        model = MLP_MIL_model_simple(patch_input_dim=patch_input_dim+clinical_length, hyperparams=hyperparams,\n",
    "                                     backbone_indicator=True, backbone=backbone)\n",
    "    if model_type == 'MLP_MIL_model2':\n",
    "        model = MLP_MIL_model2(patch_input_dim=patch_input_dim+clinical_length, hyperparams=hyperparams, \n",
    "                               backbone_indicator=True, backbone=backbone)\n",
    "\n",
    "    #model = GatedAttention(patch_input_dim)\n",
    "    model.apply(weights_init)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', factor=0.5, patience=40,\n",
    "                                                              verbose=True, threshold=0.001, threshold_mode='abs')\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    train_AUCs, test_AUCs = [], []\n",
    "    train_sensitivitys, test_sensitivitys = [], []\n",
    "    batches_mixed = 0\n",
    "    early_stopping = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        all_train_labels = []\n",
    "        all_train_preds = []\n",
    "        all_train_probs = []\n",
    "        steps = 0\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "\n",
    "        for features, label, clinical_data, number_of_nodes, LN_features in train_dataloader:\n",
    "            features, label = features.to(device), label.to(device)\n",
    "            clinical_data, number_of_nodes = clinical_data.to(device), number_of_nodes.to(device)\n",
    "            LN_features = LN_features.to(device)\n",
    "            steps += 1\n",
    "            # Forward pass\n",
    "            #output = model(features.squeeze(0))  # Remove batch dimension\n",
    "            output, max_vals, attentions, classifications = model(features, clinical_data, number_of_nodes, label, LN_features)\n",
    "            output = output.squeeze(1)\n",
    "\n",
    "            # binary threshold classifications\n",
    "            # classifications = torch.where(classifications > 0.5, torch.tensor([1.]).to(device), torch.tensor([0.]).to(device))\n",
    "            # print('Classifications:', classifications)\n",
    "\n",
    "\n",
    "            #print(torch.mean(label.float()))\n",
    "            # if label == 1:\n",
    "            #     weight = torch.tensor([2.0]).to(device)\n",
    "            # if label == 0:\n",
    "            #     weight = torch.tensor([1.0]).to(device)\n",
    "\n",
    "            loss = criterion(output, label.float()) #*weight\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # # Backward pass and optimization\n",
    "            # optimiser.zero_grad()\n",
    "            # loss.backward()\n",
    "            # optimiser.step()\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            if steps % accumulation_steps == 1:\n",
    "                optimiser.step()\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "            # Apply threshold to determine predicted class\n",
    "            #predicted_probs = F.softmax(output, dim=1)[:, 1]  # Probability of class 1 (positive)\n",
    "            #predicted_probs = torch.sigmoid(output)\n",
    "\n",
    "            predicted_probs = output\n",
    "            classifications_class = (classifications >= threshold).long()\n",
    "            #predicted_probs = 0.6*max_vals + 0.35*classifications_class + 0.05*attentions\n",
    "            predicted_class = (predicted_probs >= threshold).long()\n",
    "\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_train_labels.extend(label.cpu().numpy())\n",
    "            all_train_preds.extend(predicted_class.cpu().numpy())\n",
    "            all_train_probs.extend(predicted_probs.tolist())\n",
    "            # random_int = np.random.randint(1, 20)\n",
    "            # if random_int == 1:\n",
    "            #     rdn_idx  = np.random.randint(0, len(features))\n",
    "            #     print(rdn_idx, 'label (train)', label[rdn_idx].item(), 'output', output[rdn_idx].item(), 'predicted class', predicted_class[rdn_idx].item(), 'max', max_vals[rdn_idx].item(), 'attention', attentions[rdn_idx].item(), 'classification', classifications[rdn_idx].item(), 'class binary', classifications_class[rdn_idx].item(), 'number of nodes', number_of_nodes[rdn_idx].item()) # 'reweighted prediction', predicted_probs[rdn_idx].item())\n",
    "\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "        lr_scheduler.step(train_loss/len(train_dataloader))\n",
    "        if epoch % 5 == 0 or epoch + 20 > num_epochs-1:\n",
    "            print('Learning rate:', optimiser.param_groups[0]['lr'])\n",
    "        train_losses.append(train_loss/len(train_dataloader))\n",
    "        train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
    "        train_auc = roc_auc_score(all_train_labels, all_train_preds)\n",
    "        train_AUCs.append(train_auc)\n",
    "        train_bal_accuracy = balanced_accuracy_score(all_train_labels, all_train_preds)\n",
    "        train_confusion_matrix = confusion_matrix(all_train_labels, all_train_preds)\n",
    "        tn, fp, fn, tp = confusion_matrix(all_train_labels, all_train_preds).ravel()\n",
    "        # Compute sensitivity (recall) and specificity\n",
    "        train_sensitivity = tp / (tp + fn)\n",
    "        train_specificity = tn / (tn + fp)\n",
    "        train_sensitivitys.append(train_sensitivity)\n",
    "\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train: Loss: {train_loss/len(train_dataloader):.4f}, Accuracy: {train_accuracy:.4f}, Balanced Accuracy: {train_bal_accuracy:.4f}, AUC: {train_auc:.4f}, Sensitivity: {train_sensitivity:.4f}, Specificity: {train_specificity:.4f}')\n",
    "        print(f'Train Confusion Matrix:')\n",
    "        print(train_confusion_matrix)\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        all_test_labels = []\n",
    "        all_test_preds = []\n",
    "        all_test_probs = []\n",
    "        with torch.no_grad():\n",
    "            for features, label, clinical_data, number_of_nodes, LN_features in test_dataloader:\n",
    "                features, label = features.to(device), label.to(device)\n",
    "                clinical_data, number_of_nodes = clinical_data.to(device), number_of_nodes.to(device)\n",
    "                LN_features = LN_features.to(device)\n",
    "                #output = model(features.squeeze(0))  # Remove batch dimension\n",
    "                output, max_vals, attentions, classifications = model(features, clinical_data, number_of_nodes, label, LN_features)\n",
    "                output = output.squeeze(1)\n",
    "\n",
    "                #output = output.squeeze(0)\n",
    "                loss = criterion(output, label.float())\n",
    "                test_loss += loss.item()\n",
    "\n",
    "\n",
    "                # Store predictions and labels\n",
    "                #predicted_probs = F.softmax(output, dim=1)[:, 1]  # Probability of class 1 (positive)\n",
    "                #predicted_probs = torch.sigmoid(output)\n",
    "                predicted_probs = output\n",
    "                classifications_class = (classifications >= threshold).long()\n",
    "                #predicted_probs = 0.6*max_vals + 0.35*classifications_class + 0.05*attentions\n",
    "                predicted_class = (predicted_probs >= threshold).type(torch.long)\n",
    "                all_test_labels.extend(label.cpu().numpy())\n",
    "                all_test_preds.extend(predicted_class.cpu().numpy())\n",
    "                all_test_probs.extend(predicted_probs.cpu().numpy())\n",
    "\n",
    "                # random_int = np.random.randint(1, 8)\n",
    "                # if random_int == 1:\n",
    "                #     rdn_idx  = np.random.randint(0, len(features))\n",
    "                #     print(rdn_idx, 'label (test)', label[rdn_idx].item(), 'output', output[rdn_idx].item(), 'predicted class', predicted_class[rdn_idx].item(), 'max', max_vals[rdn_idx].item(), 'attention', attentions[rdn_idx].item(), 'classification', classifications[rdn_idx].item(), 'class binary', classifications_class[rdn_idx].item(), 'number of nodes', number_of_nodes[rdn_idx].item()) #'reweighted prediction', predicted_probs[rdn_idx].item())\n",
    "                # \n",
    "\n",
    "        test_losses.append(test_loss/len(test_dataloader))\n",
    "        test_accuracy = accuracy_score(all_test_labels, all_test_preds)\n",
    "        test_auc = roc_auc_score(all_test_labels, all_test_preds)\n",
    "        test_AUCs.append(test_auc)\n",
    "        test_bal_accuracy = balanced_accuracy_score(all_test_labels, all_test_preds)\n",
    "        test_confusion_matrix = confusion_matrix(all_test_labels, all_test_preds)\n",
    "        tn, fp, fn, tp = confusion_matrix(all_test_labels, all_test_preds).ravel()\n",
    "        # Compute sensitivity (recall) and specificity\n",
    "        test_sensitivity = tp / (tp + fn)\n",
    "        test_specificity = tn / (tn + fp)\n",
    "        test_sensitivitys.append(test_sensitivity)\n",
    "        #if epoch % 5 == 0 or epoch + 20 > num_epochs-1:\n",
    "        print(f'Test: Loss: {test_loss/len(test_dataloader):.4f}, Accuracy: {test_accuracy:.4f}, Balanced Accuracy: {test_bal_accuracy:.4f}, AUC: {test_auc:.4f}, Sensitivity: {test_sensitivity:.4f}, Specificity: {test_specificity:.4f}')\n",
    "        print('Test Confusion Matrix:')\n",
    "        print(test_confusion_matrix)\n",
    "        # Wait for GPU to cool down for 10 seconds\n",
    "        time.sleep(10)\n",
    "\n",
    "        if epoch == 0:\n",
    "            test_labels = np.array(all_test_labels)\n",
    "\n",
    "        if tp >= 10 and fp <= 10:\n",
    "            # error analysis\n",
    "            best_test_preds.append(all_test_preds)\n",
    "            best_test_probs.append(all_test_probs)\n",
    "            print('number of preds logged:', len(best_test_probs))\n",
    "            error_analysis(np.array(best_test_probs), test_labels, results_path, threshold)\n",
    "            num_logged += 1\n",
    "            if tp > best_score['TP'] or (tp >= best_score['TP'] and fp < best_score['FP']) or (tp >= best_score['TP'] and fp <= best_score['FP'] and train_sensitivity > best_score['Train_Sensitivity']):\n",
    "                best_score['TP'] = tp\n",
    "                best_score['FP'] = fp\n",
    "                best_score['Train_Sensitivity'] = train_sensitivity\n",
    "                print('Saving model with TP:', tp, 'and FP:', fp, 'at epoch:', epoch)\n",
    "                training_results = {'train_losses': train_losses, 'test_losses': test_losses, 'train_AUCs': train_AUCs, 'test_AUCs': test_AUCs, 'train_sensitivitys': train_sensitivitys, 'test_sensitivitys': test_sensitivitys,\n",
    "                                    'all test labels': all_test_labels, 'all test probs': all_test_probs}\n",
    "                torch.save({\"state_dict\": model.state_dict(), \"training_results\": training_results,\n",
    "                            \"hyperparams\": hyperparams, \"train_test_split\": train_test_split_dict}, save_results_path)\n",
    "                calibration_curve_and_distribution(all_train_labels, all_train_probs, 'Train', results_path, 'saved_result_' + str(Run), save=True)\n",
    "                calibration_curve_and_distribution(all_test_labels, all_test_probs, 'Test', results_path, 'saved_result_' + str(Run), save=True)\n",
    "\n",
    "                # plot results at this stage (updating until the best run)\n",
    "                plot_MLP_results(training_results, hyperparams, results_path=results_path, filename='MLP_training_results_run_{}.png'.format(Run))\n",
    "\n",
    "\n",
    "\n",
    "        if epoch == num_epochs-1:\n",
    "            calibration_curve_and_distribution(all_train_labels, all_train_probs, 'Train', results_path, Run)\n",
    "            calibration_curve_and_distribution(all_test_labels, all_test_probs, 'Test', results_path, Run)\n",
    "\n",
    "        # log to wandb\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"Test Loss\": test_loss/len(test_dataloader),\n",
    "                \"Test Accuracy\": test_accuracy,\n",
    "                \"Test AUC\": test_auc,\n",
    "                \"Test Sensitivity\": test_sensitivity,\n",
    "                \"Test Specificity\": test_specificity,\n",
    "                \"Test TP\": tp,\n",
    "                \"Test FP\": fp,\n",
    "                \"Train Loss\": train_loss/len(train_dataloader),\n",
    "                \"Train Accuracy\": train_accuracy,\n",
    "                \"Train AUC\": train_auc,\n",
    "                \"Train Sensitivity\": train_sensitivity,\n",
    "                \"Train Specificity\": train_specificity,\n",
    "                \"Max Test AUC\": np.max(test_AUCs),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch > 25 and test_auc < 0.7:\n",
    "            early_stopping+=1\n",
    "            if early_stopping > 15 and test_auc < 0.6 and num_logged == 0:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "            if early_stopping > 25 and test_auc < 0.65 and num_logged == 0:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "\n",
    "            if early_stopping > 50 and test_auc < 0.7 and num_logged <= 1:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "\n",
    "\n",
    "    # save test preds and probs\n",
    "    np.save(results_path + '//best_test_preds.npy', np.array(best_test_preds))\n",
    "    np.save(results_path + '//best_test_probs.npy', np.array(best_test_probs))\n",
    "\n",
    "    print('Batches mixed:', batches_mixed, 'out of', len(train_dataloader)*num_epochs, 'percentage:', batches_mixed/(len(train_dataloader)*num_epochs))\n",
    "    print(hyperparams)\n",
    "    print('Time taken:', perf_counter() - time_start)\n",
    "    # Wait for GPU to cool down after each model run\n",
    "    print(f\"Cooling down for 5 mins...\")\n",
    "    time.sleep(60*5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T21:20:27.486012200Z",
     "start_time": "2024-11-05T21:20:27.465974800Z"
    }
   },
   "id": "96e691bf1fa22375"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Start sweep job.\n",
    "wandb.agent(sweep_id, function=main, count=100)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "942340ec73451a41"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
