{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Create Grad-CAM heatmaps for the VAE-MLP model for the largest short-axis diameter LNs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcfc3d9536213e30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from time import perf_counter\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, balanced_accuracy_score, confusion_matrix\n",
    "from scipy.spatial.distance import cdist\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "import nibabel as nib\n",
    "import wandb\n",
    "import random\n",
    "import pickle\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils.datasets import Load_Latent_Vectors, LoadImages, prepare_VAE_MLP_joint_data\n",
    "from utils.utility_code import get_single_scan_file_list, get_class_distribution, weights_init, plot_MLP_results, error_analysis\n",
    "from models.MLP_model import MLP_MIL_model_simple, MLP_MIL_model2\n",
    "from models.VAE_2D_model import VAE_2D\n",
    "from utils.train_and_test_functions import mixup_patient_data, mixup_batch, process_batch_with_noise, calibration_curve_and_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# custom dataloader to give original MRI patches, VAE latent vectors, and clinical data\n",
    "def patient_test_data(patient_id):\n",
    "    add_images = True\n",
    "    indices = patient_slices_dict[patient_id]\n",
    "    if len(indices) > max_node_slices:\n",
    "        mask_sizes = mask_sizes[patient_id]\n",
    "        sizes = sorted(enumerate(mask_sizes), key=lambda x: x[1], reverse=True)\n",
    "        biggest_n_mask_idx = [i for i, size in sizes[:self.max_nodes]]\n",
    "        indices = sorted([indices[i] for i in biggest_n_mask_idx])\n",
    "    \n",
    "    # add clinical data to the latent vectors\n",
    "    patient_indicator = False\n",
    "    patient_options = []\n",
    "    if \"patient\" in clinical_data_options:\n",
    "        patient_indicator = True\n",
    "        patient_options.append('age_scaled')\n",
    "        patient_options.append('sex_numeric')\n",
    "    if \"T_stage\" in clinical_data_options:\n",
    "        patient_indicator = True\n",
    "        patient_options.append('TumourLabel_numeric')\n",
    "    \n",
    "    if patient_options == []:\n",
    "        patient_clinical_data = None\n",
    "    else:\n",
    "        patient_clinical_data = cohort1[cohort1['shortpatpseudoid'] == patient_id.split('_')[0]][patient_options].values.tolist()\n",
    "        \n",
    "    with open(r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\border_metrics.pkl\", 'rb') as f:\n",
    "        compactness, convexity = pickle.load(f)\n",
    "        \n",
    "    LN_features = []\n",
    "    images = np.zeros((len(indices), 1, 32, 32))\n",
    "    for i, index in enumerate(indices):\n",
    "        file_name = all_files_list[index]\n",
    "        pat_id = patient_id.split('_')[0]\n",
    "        node_number = float(file_name.split('//')[1].split('_')[6])\n",
    "        # print(file_name, node_number)\n",
    "        long, short, ratio = short_long_axes_dict[pat_id][node_number]\n",
    "        mask_file = file_name.replace('mri', 'mask')\n",
    "        # compactness = compactness[mask_file]\n",
    "        # convexity = convexity[mask_file]\n",
    "        # print(long, short, ratio, compactness, convexity)\n",
    "    \n",
    "        node_indicator = False\n",
    "        node_options = []\n",
    "        if \"size\" in clinical_data_options:\n",
    "            node_indicator = True\n",
    "            node_options.append(long)\n",
    "            node_options.append(short)\n",
    "            node_options.append(ratio)\n",
    "        # if \"border\" in clinical_data_options:\n",
    "        #     node_indicator = True\n",
    "        #     node_options.append(compactness)\n",
    "        #     node_options.append(convexity)\n",
    "    \n",
    "        LN_features.append(node_options)\n",
    "\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        img_dir = r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1_2D_slices/\"\n",
    "        img = nib.load(img_dir + file_name).get_fdata()\n",
    "        img = transform(img)\n",
    "        images[i] = img\n",
    "    \n",
    "    \n",
    "    LN_features = np.array(LN_features)\n",
    "    if patient_indicator == True and (\"size\" in clinical_data_options):\n",
    "        patient_clinical_data = patient_clinical_data[0] + LN_features[np.argmax(LN_features[:, 0])].tolist() # add the data for node with max long axis ratio (and corresponding short/ratio/compactness/convexity) to patient level clinical data\n",
    "    if patient_indicator == True and (\"border\" in clinical_data_options) and (\"size\" not in clinical_data_options):\n",
    "        patient_clinical_data = patient_clinical_data[0] + LN_features[np.argmin(LN_features[:, 0])].tolist() # add min compactness node data to patient level clinical data\n",
    "    if patient_indicator == True and node_indicator == False:\n",
    "        patient_clinical_data = patient_clinical_data[0]\n",
    "    if patient_indicator == False and node_indicator == True:\n",
    "        if \"size\" in clinical_data_options:\n",
    "            patient_clinical_data = LN_features[np.argmax(LN_features[:, 0])].tolist()\n",
    "        if (\"border\" in clinical_data_options) and (\"size\" not in clinical_data_options):\n",
    "            patient_clinical_data = LN_features[np.argmin(LN_features[:, 0])].tolist()\n",
    "    if patient_indicator == False and node_indicator == False:\n",
    "        patient_clinical_data = []\n",
    "    \n",
    "    label = patient_labels_dict[patient_id]\n",
    "    \n",
    "    if add_images == True:\n",
    "        number_of_nodes = len(images)\n",
    "        if len(images) < max_node_slices:\n",
    "            #print(LN_features.shape, label, patient_clinical_data, number_of_nodes)\n",
    "            images = np.concatenate((images, np.zeros((max_node_slices - len(images), 1, 32, 32))), axis=0)\n",
    "            LN_features = np.concatenate((LN_features, np.ones((max_node_slices - len(LN_features), LN_features.shape[1]))*0.5), axis=0)\n",
    "            #print(images.shape, LN_features.shape)\n",
    "    \n",
    "        return torch.tensor(LN_features, dtype=torch.float32), torch.tensor(label, dtype=torch.long), torch.tensor(patient_clinical_data, dtype=torch.float32), torch.tensor(number_of_nodes, dtype=torch.float32), torch.tensor(images, dtype=torch.float32)\n",
    "\n",
    "# LN_features, label, patient_clinical_data, num_nodes, images = patient_test_data(test_ids[0])\n",
    "# print(LN_features.shape, label, patient_clinical_data, num_nodes, images.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-18T11:02:50.015477900Z"
    }
   },
   "id": "5ce7c46d185c5916"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "IMAGE_DIR = r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1_2D_slices\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cohort1 = pd.read_excel(r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1.xlsx\")\n",
    "latent_vectors = np.load(r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE2_results\\latent_vectors_36.npy\")\n",
    "\n",
    "all_files_list = ['\\mri' + '//' + f for f in os.listdir(IMAGE_DIR + '\\mri')] + ['\\mri_aug' + '//' + f  for f in os.listdir(IMAGE_DIR + '\\mri_aug')]\n",
    "all_files_list.sort()\n",
    "all_files_list = get_single_scan_file_list(all_files_list, IMAGE_DIR, cohort1)\n",
    "\n",
    "\n",
    "VAE_params_path = r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE2_results\\VAE_36.pt\"\n",
    "checkpoint = torch.load(VAE_params_path)\n",
    "train_test_split_dict = checkpoint['train_test_split']\n",
    "train_ids = train_test_split_dict['train']\n",
    "test_ids = train_test_split_dict['test']\n",
    "hyperparams = checkpoint['hyperparams']\n",
    "vae_model = VAE_2D(hyperparams)\n",
    "vae_model = vae_model.to(device)\n",
    "# Load the saved checkpoint\n",
    "vae_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "# Put the model into evaluation mode if you're not training anymore\n",
    "vae_model.eval()\n",
    "encoder = vae_model.encoder\n",
    "\n",
    "\n",
    "mlp_checkpoint = torch.load(r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\MLP_Results\\best_model\\MLP_62.pt\")\n",
    "mlp_hyperparams = mlp_checkpoint['hyperparams']\n",
    "print(mlp_hyperparams)\n",
    "clinical_data_options = mlp_hyperparams['clinical_data_options']\n",
    "clinical_length = 0\n",
    "if \"size\" in clinical_data_options:\n",
    "    clinical_length += 3\n",
    "if \"border\" in clinical_data_options:\n",
    "    clinical_length += 2\n",
    "print(\"clinical_length\", clinical_length)\n",
    "mlp_model = MLP_MIL_model2(patch_input_dim=400+clinical_length, hyperparams=mlp_hyperparams, grad_cam=True)\n",
    "mlp_model = mlp_model.to(device)\n",
    "mlp_model.load_state_dict(mlp_checkpoint[\"state_dict\"])\n",
    "mlp_model.eval()\n",
    "\n",
    "max_node_slices = mlp_hyperparams['max_node_slices']\n",
    "n_synthetic = 0\n",
    "oversample = 1\n",
    "\n",
    "\n",
    "\n",
    "first_time = False\n",
    "if first_time:\n",
    "    patient_slices_dict, patient_labels_dict, patient_file_names_dict, short_long_axes_dict, mlp_train_ids, test_ids, mlp_train_labels, test_labels, train_images, test_images, train_test_split_dict, mask_sizes = prepare_VAE_MLP_joint_data(first_time_train_test_split=False, train_ids=train_ids, test_ids=test_ids, num_synthetic=n_synthetic, oversample_ratio=oversample)\n",
    "    data_dictionaries = {\"slices\": patient_slices_dict, \"labels\": patient_labels_dict, \"files\": patient_file_names_dict, \"sizes\": short_long_axes_dict, \"mask_sizes\": mask_sizes, \"mlp_train_ids\": mlp_train_ids, \"test_ids\": test_ids, \"mlp_train_labels\": mlp_train_labels, \"test_labels\": test_labels, \"train_images\": train_images, \"test_images\": test_images, \"train_test_split_dict\": train_test_split_dict}\n",
    "    torch.save(data_dictionaries, r'C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\MLP_Results\\data_dictionaries.pth')\n",
    "else:\n",
    "    data_dictionaries = torch.load(r'C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\MLP_Results\\data_dictionaries.pth')\n",
    "    patient_slices_dict = data_dictionaries[\"slices\"]\n",
    "    patient_labels_dict = data_dictionaries[\"labels\"]\n",
    "    patient_file_names_dict = data_dictionaries[\"files\"]\n",
    "    short_long_axes_dict = data_dictionaries[\"sizes\"]\n",
    "    mask_sizes = data_dictionaries[\"mask_sizes\"]\n",
    "    mlp_train_ids = data_dictionaries[\"mlp_train_ids\"]\n",
    "    test_ids = data_dictionaries[\"test_ids\"]\n",
    "    mlp_train_labels = data_dictionaries[\"mlp_train_labels\"]\n",
    "    test_labels = data_dictionaries[\"test_labels\"]\n",
    "    train_images = data_dictionaries[\"train_images\"]\n",
    "    test_images = data_dictionaries[\"test_images\"]\n",
    "    train_test_split_dict = data_dictionaries[\"train_test_split_dict\"]\n",
    "\n",
    "\n",
    "train_dataset = LoadImages(main_dir=IMAGE_DIR + '/', files_list=train_images)\n",
    "test_dataset = LoadImages(main_dir=IMAGE_DIR + '/', files_list=test_images)\n",
    "train_loader = DataLoader(train_dataset, 1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, 1, shuffle=False)\n",
    "\n",
    "\n",
    "original_images = []\n",
    "reconstructed_images = []\n",
    "\n",
    "# random seed\n",
    "random.seed(40)\n",
    "\n",
    "\n",
    "\n",
    "#random_indices = random.sample(range(59), 16)\n",
    "examples = [6, 12, 14, 33, 37, 41, 43, 21]\n",
    "\n",
    "# # With no gradients, gather reconstructions\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, data in enumerate(test_loader):\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "important_latent_variables = {i:0 for i in range(400)}\n",
    "#for patient_idx in examples:\n",
    "\n",
    "for patient_idx, id in enumerate(test_ids):\n",
    "#for patient_idx in examples:\n",
    "    print('Patient ID:', id, 'Patient index:', patient_idx)\n",
    "    if id == 42:\n",
    "        pass\n",
    "    else:\n",
    "        LN_features, label, clinical_data, num_nodes, images = patient_test_data(test_ids[patient_idx])\n",
    "        #images = images[0].unsqueeze(0)  # Unsqueeze to make it a batch of 1\n",
    "        #LN_features = LN_features[0].unsqueeze(0)\n",
    "\n",
    "        images = images.float().to(device)\n",
    "        images.requires_grad_()\n",
    "        \n",
    "        # Hook function to save feature maps\n",
    "        def save_feature_maps(module, input, output):\n",
    "            # Saving the output feature map from this layer\n",
    "            feature_maps.append(output)\n",
    "            output.retain_grad()  # Ensure gradients are stored for this tensor\n",
    "\n",
    "\n",
    "        # Register the hook to the last convolutional layer\n",
    "        feature_maps = []\n",
    "        last_conv_layer = vae_model.encoder[4].conv[0] \n",
    "        hook = last_conv_layer.register_forward_hook(save_feature_maps)\n",
    "        \n",
    "\n",
    "        # Pass the data through the model to get reconstructions\n",
    "        vae_model.eval()\n",
    "        vae_model.zero_grad()\n",
    "        \n",
    "        recons, mu, log_var = vae_model(images.float().to(device))\n",
    "\n",
    "        mu = mu.squeeze() \n",
    "        \n",
    "        LN_features = torch.tensor(LN_features, dtype=torch.float32).to(device)\n",
    "        print('LN_features:', LN_features.shape, 'mu:', mu.shape)\n",
    "        features = torch.cat((mu, LN_features), axis=1)\n",
    "\n",
    "        label, clinical_data, num_nodes, features = label.unsqueeze(dim=0).to(device), clinical_data.unsqueeze(dim=0).to(device), num_nodes.unsqueeze(dim=0).to(device), features.unsqueeze(dim=0)   \n",
    "        print('Patient ID:', train_ids[patient_idx], 'Label:', label.squeeze().item(), 'Num nodes:', num_nodes.squeeze().item())\n",
    "\n",
    "        #(25, 403)\n",
    "        output, max_vals, refined_LNM_predictions, patch_features = mlp_model(features, clinical_data, num_nodes, label)\n",
    "\n",
    "    \n",
    "    \n",
    "        print('Max prediction:', max_vals)\n",
    "        print('prediction', output, 'mlp pred', refined_LNM_predictions)\n",
    "        #print('patch_predictions', patch_features)\n",
    "        #max_node_index = torch.argmax(patch_features, dim=1)\n",
    "        node_features = LN_features.detach().cpu().numpy()\n",
    "        node_features = node_features[:int(num_nodes.squeeze().item()), :] \n",
    "        max_node_index = np.argmax(node_features[:, 0])\n",
    "        print('Max node index: ', max_node_index, 'short-axis diameter: ', node_features)\n",
    "\n",
    "        biggest_size = 0.35\n",
    "        if label == 1:\n",
    "            for i in range(int(num_nodes.item())):\n",
    "                if node_features[i][0] > biggest_size:\n",
    "                    biggest_size = node_features[i][0]\n",
    "                    max_node_index = i\n",
    "                    print('Max node index:', max_node_index, 'short-axis diameter:', node_features[i][0])\n",
    "                else:\n",
    "                    continue\n",
    "                # run through maps 80 x (8x8) and average gradient\n",
    "                gradients = []\n",
    "                image_idx = max_node_index\n",
    "                print('Max node index:', max_node_index, feature_maps[0].shape)\n",
    "                for i in range(80):\n",
    "                    for j in range(8):\n",
    "                        for k in range(8):\n",
    "                            images.grad = None\n",
    "                            vae_model.eval()\n",
    "                            vae_model.zero_grad()\n",
    "                            feature_maps = []\n",
    "                            recons, mu, log_var = vae_model(images.float().to(device))\n",
    "                            feature_maps = feature_maps[0][image_idx].squeeze()\n",
    "                            feature_maps[i][j][k].backward(retain_graph=True)\n",
    "                            gradients.append(images.grad)\n",
    "                gradients = torch.mean(torch.stack(gradients), dim=0)\n",
    "                \n",
    "\n",
    "                print('clinical data:', clinical_data)\n",
    "\n",
    "                grad_cam_map = F.relu(gradients.squeeze())  # Apply ReLU to focus on \n",
    "        \n",
    "                # Normalize heatmap\n",
    "                heatmap = grad_cam_map.detach().cpu().numpy()\n",
    "                heatmap = heatmap[max_node_index]\n",
    "                heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "                heatmap = (1 - heatmap)  # Invert heatmap\n",
    "                heatmap = (heatmap / heatmap.max()) * 255  # Scale to [0, 255]\n",
    "                heatmap = np.uint8(heatmap)\n",
    "        \n",
    "                # Overlay heatmap onto the original image\n",
    "                original_image = images.detach().squeeze().cpu().numpy()  \n",
    "                original_image = original_image[max_node_index]\n",
    "                original_image = (original_image / original_image.max()) * 255  # Scale to [0, 255]\n",
    "                original_image = np.uint8(original_image)  # Convert to uint8 for OpenCV compatibility\n",
    "                # Scale heatmap to [0, 255] and convert to uint8        \n",
    "                heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)  # Convert to color map for better visualization\n",
    "                overlayed_img = cv2.addWeighted(cv2.cvtColor(original_image, cv2.COLOR_GRAY2BGR), 0.5, heatmap, 0.5, 0)\n",
    "        \n",
    "                # Display\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.title(\"Original Image\", fontsize=18)\n",
    "                \n",
    "                plt.imshow(original_image, cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.title(\"Grad-CAM Heatmap\", fontsize=18)\n",
    "                plt.imshow(overlayed_img)\n",
    "                plt.axis('off')\n",
    "                # save the plot\n",
    "                plt.savefig(r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\Grad-CAM\\Grad-CAM_\" + str(patient_idx) + \"_\" + str(max_node_index) + \".png\")\n",
    "                plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-18T11:02:50.015477900Z"
    }
   },
   "id": "708b959f594d7185"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
