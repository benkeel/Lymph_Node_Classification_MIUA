{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter sweep for MLP model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9e7213af0098531"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-05T14:09:29.612414800Z",
     "start_time": "2024-11-05T14:09:20.371971Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from time import perf_counter\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, balanced_accuracy_score, confusion_matrix\n",
    "from scipy.spatial.distance import cdist\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "import nibabel as nib\n",
    "import wandb\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils.datasets import Load_Latent_Vectors, LoadImages, prepare_VAE_MLP_joint_data\n",
    "from utils.utility_code import get_single_scan_file_list, get_class_distribution, weights_init, plot_MLP_results, error_analysis\n",
    "from models.MLP_model import MLP_MIL_model_simple, MLP_MIL_model2\n",
    "from models.VAE_2D_model import VAE_2D\n",
    "from utils.train_and_test_functions import mixup_patient_data, mixup_batch, process_batch_with_noise, calibration_curve_and_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c014f4d9b92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best so far: {'num_epochs': 200, 'threshold': 0.4049798489191535, 'num_synthetic': 30, 'oversample': 1.5, 'batch_size': 64, 'lr': 0.004089429701418951, 'weight_decay': 0.08306021271710541, 'accumulation_steps': 3, 'patch_hidden_dim': 2048, 'max_node_slices': 15, 'model_type': 'MLP_MIL_model2'}\n",
    "\n",
    "# Define sweep configuration\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"name\": \"sweep2\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"Max Test AUC\"},\n",
    "    \"parameters\": {\n",
    "        #\"dataset_version\": {\"values\": dataset_version},\n",
    "        \"num_synthetic\": {\"values\": [10, 15, 20, 25, 30]},\n",
    "        \"oversample\": {\"values\": [1, 1.25, 1.5, 2]},\n",
    "        \"max_node_slices\": {\"values\": [15, 20, 25, 30]},\n",
    "        \"threshold\": {\"max\": 0.55, \"min\": 0.4},\n",
    "        \"batch_size\": {\"values\": [64, 100, 128, 150, 256, 300]},\n",
    "        \"lr\": {\"max\": 0.009, \"min\": 0.002},\n",
    "        \"weight_decay\": {\"max\": 0.2, \"min\": 0.04},\n",
    "        \"accumulation_steps\": {\"values\": [1, 2, 3, 4, 5]},\n",
    "        \"patch_hidden_dim\": {\"values\": [128, 256, 512, 1024, 1536, 2048, 2560]},\n",
    "        \"patient_hidden_dim\": {\"values\": [16, 24, 32, 36, 46, 64, 96, 128]},\n",
    "        \"patch_dropout\": {\"values\": [0.2, 0.3, 0.4]},\n",
    "        \"patient_dropout\": {\"values\": [0.2, 0.3, 0.4]},\n",
    "        \"alpha\": {\"values\": [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]}, # \"alpha*max_vals + (0.9-alpha)*classifications + 0.1*attentions\n",
    "        \"mixup\": {\"values\": [True, False]},\n",
    "        \"attention_indicator\": {\"values\": [True, False]},\n",
    "        \"model_type\": {\"values\": [\"MLP_MIL_model2\"]}, #\"MLP_MIL_model_simple\",\n",
    "        \"clinical_data_options\": {\"values\": [[\"T_stage\", \"size\", \"border\", \"patient\"], [\"T_stage\", \"size\", \"border\"], [\"T_stage\", \"size\", \"patient\"], [\"T_stage\", \"border\", \"patient\"], [\"size\", \"border\", \"patient\"], [\"T_stage\", \"size\"], [\"T_stage\", \"border\"], [\"T_stage\", \"patient\"], [\"size\", \"border\"], [\"size\", \"patient\"], [\"border\", \"patient\"], [\"T_stage\"], [\"size\"], [\"border\"], [\"patient\"]]},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialize sweep by passing in config.\n",
    "# Provide a name of the project.\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"MLP-bayesian-sweep-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e81fde47e21e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ea782ef7ac0cdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T14:09:33.021078Z",
     "start_time": "2024-11-05T14:09:33.018335800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Current Best\n",
    "# run = 14\n",
    "# # Hyperparams\n",
    "# patch_input_dim = 259\n",
    "# patch_hidden_dim = 1024\n",
    "# patch_output_dim = 1\n",
    "# attention_dim = 1\n",
    "# threshold = 0.5\n",
    "# output_dim = 1\n",
    "# num_epochs = 300\n",
    "# lr = 5e-5\n",
    "# weight_decay = 1e-3\n",
    "# batch_size = 64\n",
    "# n_synthetic = 30\n",
    "# oversample = 2\n",
    "# accumulation_steps = 2\n",
    "# max_node_slices = 20\n",
    "# \n",
    "# hyperparams = {'patch_hidden_dim': patch_hidden_dim, 'num_epochs': num_epochs, 'lr': lr, 'batch_size': batch_size, 'n_synthetic': n_synthetic, 'oversample': oversample, 'accumulation_steps': accumulation_steps, 'max_node_slices': max_node_slices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ec8a40636afddd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T14:09:33.032284200Z",
     "start_time": "2024-11-05T14:09:33.021078Z"
    }
   },
   "outputs": [],
   "source": [
    "save_data = False\n",
    "if save_data:\n",
    "    n_synthetic = 30\n",
    "    oversample = 2\n",
    "    time = perf_counter()\n",
    "    \n",
    "    IMAGE_DIR = r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1_2D_slices\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    cohort1 = pd.read_excel(r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1.xlsx\")\n",
    "    latent_vectors = np.load(r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE2_results\\latent_vectors_36.npy\")\n",
    "    \n",
    "    all_files_list = ['\\mri' + '//' + f for f in os.listdir(IMAGE_DIR + '\\mri')] + ['\\mri_aug' + '//' + f  for f in os.listdir(IMAGE_DIR + '\\mri_aug')]\n",
    "    all_files_list.sort()\n",
    "    all_files_list = get_single_scan_file_list(all_files_list, IMAGE_DIR, cohort1)\n",
    "    \n",
    "    VAE_params_path = r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE2_results\\VAE_36.pt\"\n",
    "    checkpoint = torch.load(VAE_params_path)\n",
    "    train_test_split_dict = checkpoint['train_test_split']\n",
    "    train_ids = train_test_split_dict['train']\n",
    "    test_ids = train_test_split_dict['test']\n",
    "    patient_slices_dict, patient_labels_dict, patient_file_names_dict, short_long_axes_dict, mlp_train_ids, test_ids, mlp_train_labels, test_labels, train_images, test_images, train_test_split_dict, mask_sizes = prepare_VAE_MLP_joint_data(first_time_train_test_split=False, train_ids=train_ids, test_ids=test_ids, num_synthetic=n_synthetic, oversample_ratio=oversample)\n",
    "    \n",
    "    clinical_data_options = [\"T_stage\", \"size\", \"border\", \"patient\"]\n",
    "    train_dataset = Load_Latent_Vectors(patient_slices_dict, latent_vectors, patient_labels_dict, mlp_train_ids, cohort1, all_files_list, short_long_axes_dict, mask_sizes, clinical_data_options, max_nodes=max_node_slices)\n",
    "    #train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    #test_dataset = Load_Latent_Vectors(patient_slices_dict, latent_vectors, patient_labels_dict, test_ids, cohort1, all_files_list, short_long_axes_dict, mask_sizes, max_nodes=max_node_slices)\n",
    "    #test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "    print('Time taken:', perf_counter() - time)\n",
    "    torch.save(train_dataset, r'C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\MLP_Results\\train_dataset_30syn_2oversample.pth')\n",
    "    print('done')\n",
    "    #torch.save(test_dataset, r'C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\MLP_Results\\test_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19a04a782ca63ac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T14:09:34.466840100Z",
     "start_time": "2024-11-05T14:09:34.438578700Z"
    }
   },
   "outputs": [],
   "source": [
    "Run = 0\n",
    "best_test_preds, best_test_probs= [], []\n",
    "def main():\n",
    "    global Run, best_test_preds, best_test_probs\n",
    "    best_score = {'TP': 10, 'FP': 8, 'Train_Sensitivity': 0.6}\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    run = wandb.init()\n",
    "    Run += 1\n",
    "    print('Run:', Run)\n",
    "    num_logged = 0\n",
    "    # Hyperparams    \n",
    "    num_epochs = 200\n",
    "    #dataset_version = wandb.config.dataset_version\n",
    "    num_synthetic = wandb.config.num_synthetic\n",
    "    oversample = wandb.config.oversample\n",
    "    max_node_slices = wandb.config.max_node_slices\n",
    "    threshold = wandb.config.threshold\n",
    "    batch_size = wandb.config.batch_size\n",
    "    lr = wandb.config.lr\n",
    "    weight_decay = wandb.config.weight_decay\n",
    "    accumulation_steps = wandb.config.accumulation_steps\n",
    "    patch_hidden_dim = wandb.config.patch_hidden_dim\n",
    "    patient_hidden_dim = wandb.config.patient_hidden_dim\n",
    "    patch_dropout = wandb.config.patch_dropout\n",
    "    patient_dropout = wandb.config.patient_dropout\n",
    "    model_type = wandb.config.model_type\n",
    "    mixup = wandb.config.mixup\n",
    "    attention_indicator = wandb.config.attention_indicator\n",
    "    alpha = wandb.config.alpha\n",
    "    clinical_data_options = wandb.config.clinical_data_options\n",
    "    clinical_length = 0\n",
    "    if \"size\" in clinical_data_options:\n",
    "        clinical_length += 3\n",
    "    if \"border\" in clinical_data_options:\n",
    "        clinical_length += 2\n",
    "        \n",
    "    hyperparams = {'num_epochs': num_epochs, 'threshold': threshold, 'num_synthetic': num_synthetic, 'oversample': oversample,\n",
    "                   'batch_size': batch_size, 'lr': lr, 'weight_decay': weight_decay, 'accumulation_steps': accumulation_steps,\n",
    "                   'patch_hidden_dim': patch_hidden_dim, 'patient_hidden_dim': patient_hidden_dim,\n",
    "                   'patch_dropout': patch_dropout, 'patient_dropout': patient_dropout, 'alpha': alpha, 'mixup': mixup,\n",
    "                   'attention_indicator': attention_indicator, 'max_node_slices': max_node_slices, 'model_type': model_type, \n",
    "                   'clinical_data_options': clinical_data_options, 'device': device}\n",
    "    \n",
    "    print(hyperparams)\n",
    "    print('Device:', device)\n",
    "    \n",
    "    time_start = perf_counter()\n",
    "\n",
    "    results_path = r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\MLP_Results\"\n",
    "    save_results_path = rf\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\MLP_Results\\MLP_{Run}.pt\"\n",
    "    # Load the dataset\n",
    "    IMAGE_DIR = r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1_2D_slices\"\n",
    "    cohort1 = pd.read_excel(r\"C:\\Users\\mm17b2k.DS\\Documents\\ARCANE_Data\\Cohort1.xlsx\")\n",
    "    latent_vectors = np.load(r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE2_results\\latent_vectors_36.npy\")\n",
    "\n",
    "    all_files_list = ['\\mri' + '//' + f for f in os.listdir(IMAGE_DIR + '\\mri')] + ['\\mri_aug' + '//' + f  for f in os.listdir(IMAGE_DIR + '\\mri_aug')]\n",
    "    all_files_list.sort()\n",
    "    all_files_list = get_single_scan_file_list(all_files_list, IMAGE_DIR, cohort1)\n",
    "    \n",
    "    VAE_params_path = r\"C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\VAE2_results\\VAE_36.pt\"\n",
    "    checkpoint = torch.load(VAE_params_path)\n",
    "    train_test_split_dict = checkpoint['train_test_split']\n",
    "    train_ids = train_test_split_dict['train']\n",
    "    test_ids = train_test_split_dict['test']\n",
    "    patient_slices_dict, patient_labels_dict, patient_file_names_dict, short_long_axes_dict, mlp_train_ids, test_ids, mlp_train_labels, test_labels, train_images, test_images, train_test_split_dict, mask_sizes = prepare_VAE_MLP_joint_data(first_time_train_test_split=False, train_ids=train_ids, test_ids=test_ids, num_synthetic=num_synthetic, oversample_ratio=oversample)\n",
    "\n",
    "    #train_dataset = torch.load(r'C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\MLP_Results\\{}.pth'.format(dataset_version))\n",
    "    train_dataset = Load_Latent_Vectors(patient_slices_dict, latent_vectors, patient_labels_dict, mlp_train_ids, cohort1, all_files_list, short_long_axes_dict, mask_sizes, clinical_data_options, max_nodes=max_node_slices)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #test_dataset = torch.load(r'C:\\Users\\mm17b2k.DS\\Documents\\Python\\ARCANE_Results\\MLP_Results\\test_dataset.pth')\n",
    "    test_dataset = Load_Latent_Vectors(patient_slices_dict, latent_vectors, patient_labels_dict, test_ids, cohort1, all_files_list, short_long_axes_dict, mask_sizes, clinical_data_options, max_nodes=max_node_slices)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "    # Instantiate the model\n",
    "    if model_type == 'MLP_MIL_model_simple':\n",
    "        model = MLP_MIL_model_simple(patch_input_dim=400+clinical_length, hyperparams=hyperparams) \n",
    "    if model_type == 'MLP_MIL_model2':\n",
    "        model = MLP_MIL_model2(patch_input_dim=400+clinical_length, hyperparams=hyperparams)\n",
    "        \n",
    "    #model = GatedAttention(patch_input_dim)\n",
    "    model.apply(weights_init)\n",
    "    model.to(device)\n",
    "    \n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', factor=0.5, patience=40,\n",
    "                                                              verbose=True, threshold=0.001, threshold_mode='abs')\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    train_AUCs, test_AUCs = [], []\n",
    "    train_sensitivitys, test_sensitivitys = [], []\n",
    "    batches_mixed = 0\n",
    "    early_stopping = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        all_train_labels = []\n",
    "        all_train_preds = []\n",
    "        all_train_probs = []\n",
    "        steps = 0\n",
    "        optimiser.zero_grad()\n",
    "    \n",
    "    \n",
    "        for features, label, clinical_data, number_of_nodes in train_dataloader:\n",
    "            if mixup:\n",
    "                random_int = np.random.randint(1, 6) # random int between 1 and 3\n",
    "            else:\n",
    "                random_int = 0\n",
    "            if random_int == 1: # 1/6 chance\n",
    "                #print('Mixup')\n",
    "                batches_mixed += 1\n",
    "                \n",
    "                features_without_clinical = features[:, :, :-clinical_length]\n",
    "                features_mixed = mixup_batch(features_without_clinical) # mixup 50% of batch\n",
    "                if clinical_length > 0:\n",
    "                    features = torch.cat((features_mixed, features[:, :, -clinical_length:]), dim=2) # add clinical data back\n",
    "            if random_int == 2: # 1/6 chance\n",
    "                #print('Noise')\n",
    "                batches_mixed += 1\n",
    "                features_without_clinical = features[:, :, :-clinical_length]\n",
    "                features_noise = process_batch_with_noise(features_without_clinical) # add noise to 50% of batch\n",
    "                if clinical_length > 0:\n",
    "                    features = torch.cat((features_noise, features[:, :, -clinical_length:]), dim=2) # add clinical data back\n",
    "    \n",
    "            features, label = features.to(device), label.to(device)\n",
    "            clinical_data, number_of_nodes = clinical_data.to(device), number_of_nodes.to(device)\n",
    "            steps += 1\n",
    "            # Forward pass\n",
    "            #output = model(features.squeeze(0))  # Remove batch dimension\n",
    "            print('features shape:', features.shape, 'label shape:', label.shape, 'clinical data shape:', clinical_data.shape, 'number of nodes shape:', number_of_nodes.shape)\n",
    "            output, max_vals, attentions, classifications = model(features, clinical_data, number_of_nodes, label)\n",
    "            output = output.squeeze(1)\n",
    "    \n",
    "            # binary threshold classifications\n",
    "            # classifications = torch.where(classifications > 0.5, torch.tensor([1.]).to(device), torch.tensor([0.]).to(device))\n",
    "            # print('Classifications:', classifications)\n",
    "    \n",
    "    \n",
    "            #print(torch.mean(label.float()))\n",
    "            # if label == 1:\n",
    "            #     weight = torch.tensor([2.0]).to(device)\n",
    "            # if label == 0:\n",
    "            #     weight = torch.tensor([1.0]).to(device)\n",
    "    \n",
    "            loss = criterion(output, label.float()) #*weight\n",
    "            train_loss += loss.item()\n",
    "    \n",
    "            # # Backward pass and optimization\n",
    "            # optimiser.zero_grad()\n",
    "            # loss.backward()\n",
    "            # optimiser.step()\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            if steps % accumulation_steps == 1:\n",
    "                optimiser.step()\n",
    "                optimiser.zero_grad()\n",
    "    \n",
    "            # Apply threshold to determine predicted class\n",
    "            #predicted_probs = F.softmax(output, dim=1)[:, 1]  # Probability of class 1 (positive)\n",
    "            #predicted_probs = torch.sigmoid(output)\n",
    "    \n",
    "            predicted_probs = output\n",
    "            classifications_class = (classifications >= threshold).long()\n",
    "            #predicted_probs = 0.6*max_vals + 0.35*classifications_class + 0.05*attentions\n",
    "            predicted_class = (predicted_probs >= threshold).long()\n",
    "    \n",
    "    \n",
    "            # Store predictions and labels\n",
    "            all_train_labels.extend(label.cpu().numpy())\n",
    "            all_train_preds.extend(predicted_class.cpu().numpy())\n",
    "            all_train_probs.extend(predicted_probs.tolist())\n",
    "            # random_int = np.random.randint(1, 20)\n",
    "            # if random_int == 1:\n",
    "            #     rdn_idx  = np.random.randint(0, len(features))\n",
    "            #     print(rdn_idx, 'label (train)', label[rdn_idx].item(), 'output', output[rdn_idx].item(), 'predicted class', predicted_class[rdn_idx].item(), 'max', max_vals[rdn_idx].item(), 'attention', attentions[rdn_idx].item(), 'classification', classifications[rdn_idx].item(), 'class binary', classifications_class[rdn_idx].item(), 'number of nodes', number_of_nodes[rdn_idx].item()) # 'reweighted prediction', predicted_probs[rdn_idx].item())\n",
    "    \n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "        lr_scheduler.step(train_loss/len(train_dataloader))\n",
    "        if epoch % 5 == 0 or epoch + 20 > num_epochs-1:\n",
    "            print('Learning rate:', optimiser.param_groups[0]['lr'])\n",
    "        train_losses.append(train_loss/len(train_dataloader))\n",
    "        train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
    "        train_auc = roc_auc_score(all_train_labels, all_train_preds)\n",
    "        train_AUCs.append(train_auc)\n",
    "        train_bal_accuracy = balanced_accuracy_score(all_train_labels, all_train_preds)\n",
    "        train_confusion_matrix = confusion_matrix(all_train_labels, all_train_preds)\n",
    "        tn, fp, fn, tp = confusion_matrix(all_train_labels, all_train_preds).ravel()\n",
    "        # Compute sensitivity (recall) and specificity\n",
    "        train_sensitivity = tp / (tp + fn)\n",
    "        train_specificity = tn / (tn + fp)\n",
    "        train_sensitivitys.append(train_sensitivity)\n",
    "    \n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train: Loss: {train_loss/len(train_dataloader):.4f}, Accuracy: {train_accuracy:.4f}, Balanced Accuracy: {train_bal_accuracy:.4f}, AUC: {train_auc:.4f}, Sensitivity: {train_sensitivity:.4f}, Specificity: {train_specificity:.4f}')\n",
    "        print(f'Train Confusion Matrix:')\n",
    "        print(train_confusion_matrix)\n",
    "    \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        all_test_labels = []\n",
    "        all_test_preds = []\n",
    "        all_test_probs = []\n",
    "        with torch.no_grad():\n",
    "            for features, label, clinical_data, number_of_nodes in test_dataloader:\n",
    "                features, label = features.to(device), label.to(device)\n",
    "                clinical_data, number_of_nodes = clinical_data.to(device), number_of_nodes.to(device)\n",
    "                #output = model(features.squeeze(0))  # Remove batch dimension\n",
    "                output, max_vals, attentions, classifications = model(features, clinical_data, number_of_nodes, label)\n",
    "                output = output.squeeze(1)\n",
    "    \n",
    "                #output = output.squeeze(0)\n",
    "                loss = criterion(output, label.float())\n",
    "                test_loss += loss.item()\n",
    "    \n",
    "    \n",
    "                # Store predictions and labels\n",
    "                #predicted_probs = F.softmax(output, dim=1)[:, 1]  # Probability of class 1 (positive)\n",
    "                #predicted_probs = torch.sigmoid(output)\n",
    "                predicted_probs = output\n",
    "                classifications_class = (classifications >= threshold).long()\n",
    "                #predicted_probs = 0.6*max_vals + 0.35*classifications_class + 0.05*attentions\n",
    "                predicted_class = (predicted_probs >= threshold).type(torch.long)\n",
    "                all_test_labels.extend(label.cpu().numpy())\n",
    "                all_test_preds.extend(predicted_class.cpu().numpy())\n",
    "                all_test_probs.extend(predicted_probs.cpu().numpy())\n",
    "    \n",
    "                # random_int = np.random.randint(1, 8)\n",
    "                # if random_int == 1:\n",
    "                #     rdn_idx  = np.random.randint(0, len(features))\n",
    "                #     print(rdn_idx, 'label (test)', label[rdn_idx].item(), 'output', output[rdn_idx].item(), 'predicted class', predicted_class[rdn_idx].item(), 'max', max_vals[rdn_idx].item(), 'attention', attentions[rdn_idx].item(), 'classification', classifications[rdn_idx].item(), 'class binary', classifications_class[rdn_idx].item(), 'number of nodes', number_of_nodes[rdn_idx].item()) #'reweighted prediction', predicted_probs[rdn_idx].item())\n",
    "                # \n",
    "    \n",
    "        test_losses.append(test_loss/len(test_dataloader))\n",
    "        test_accuracy = accuracy_score(all_test_labels, all_test_preds)\n",
    "        test_auc = roc_auc_score(all_test_labels, all_test_preds)\n",
    "        test_AUCs.append(test_auc)\n",
    "        test_bal_accuracy = balanced_accuracy_score(all_test_labels, all_test_preds)\n",
    "        test_confusion_matrix = confusion_matrix(all_test_labels, all_test_preds)\n",
    "        tn, fp, fn, tp = confusion_matrix(all_test_labels, all_test_preds).ravel()\n",
    "        # Compute sensitivity (recall) and specificity\n",
    "        test_sensitivity = tp / (tp + fn)\n",
    "        test_specificity = tn / (tn + fp)\n",
    "        test_sensitivitys.append(test_sensitivity)\n",
    "        #if epoch % 5 == 0 or epoch + 20 > num_epochs-1:\n",
    "        print(f'Test: Loss: {test_loss/len(test_dataloader):.4f}, Accuracy: {test_accuracy:.4f}, Balanced Accuracy: {test_bal_accuracy:.4f}, AUC: {test_auc:.4f}, Sensitivity: {test_sensitivity:.4f}, Specificity: {test_specificity:.4f}')\n",
    "        print('Test Confusion Matrix:')\n",
    "        print(test_confusion_matrix)\n",
    "        # Wait for GPU to cool down for 10 seconds\n",
    "        time.sleep(10)\n",
    "            \n",
    "        if epoch == 0:\n",
    "            test_labels = np.array(all_test_labels)\n",
    "            \n",
    "        if tp >= 10 and fp <= 8: \n",
    "            # error analysis\n",
    "            best_test_preds.append(all_test_preds)\n",
    "            best_test_probs.append(all_test_probs)\n",
    "            print('number of preds logged:', len(best_test_probs))\n",
    "            error_analysis(np.array(best_test_probs), test_labels, results_path, threshold)\n",
    "            num_logged += 1\n",
    "        if tp >= 11 and fp <= 8:\n",
    "            if tp > best_score['TP'] or (tp >= best_score['TP'] and fp < best_score['FP']) or (tp >= best_score['TP'] and fp <= best_score['FP'] and train_sensitivity > best_score['Train_Sensitivity']):\n",
    "                best_score['TP'] = tp\n",
    "                best_score['FP'] = fp\n",
    "                best_score['Train_Sensitivity'] = train_sensitivity\n",
    "                print('Saving model with TP:', tp, 'and FP:', fp, 'at epoch:', epoch)\n",
    "                training_results = {'train_losses': train_losses, 'test_losses': test_losses, 'train_AUCs': train_AUCs, 'test_AUCs': test_AUCs, 'train_sensitivitys': train_sensitivitys, 'test_sensitivitys': test_sensitivitys,\n",
    "                 'all test labels': all_test_labels, 'all test probs': all_test_probs}\n",
    "                torch.save({\"state_dict\": model.state_dict(), \"training_results\": training_results,\n",
    "                \"hyperparams\": hyperparams, \"train_test_split\": train_test_split_dict}, save_results_path)\n",
    "                calibration_curve_and_distribution(all_train_labels, all_train_probs, 'Train', results_path, 'saved_result_' + str(Run), save=True)\n",
    "                calibration_curve_and_distribution(all_test_labels, all_test_probs, 'Test', results_path, 'saved_result_' + str(Run), save=True)\n",
    "                \n",
    "                # plot results at this stage (updating until the best run)\n",
    "                plot_MLP_results(training_results, hyperparams, results_path=results_path, filename='MLP_training_results_run_{}.png'.format(Run))\n",
    "    \n",
    "\n",
    "    \n",
    "        if epoch == num_epochs-1:\n",
    "            calibration_curve_and_distribution(all_train_labels, all_train_probs, 'Train', results_path, Run)\n",
    "            calibration_curve_and_distribution(all_test_labels, all_test_probs, 'Test', results_path, Run)\n",
    "\n",
    "        # log to wandb\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"Test Loss\": test_loss/len(test_dataloader),\n",
    "                \"Test Accuracy\": test_accuracy,\n",
    "                \"Test AUC\": test_auc,\n",
    "                \"Test Sensitivity\": test_sensitivity,\n",
    "                \"Test Specificity\": test_specificity,\n",
    "                \"Test TP\": tp,\n",
    "                \"Test FP\": fp,\n",
    "                \"Train Loss\": train_loss/len(train_dataloader),\n",
    "                \"Train Accuracy\": train_accuracy,\n",
    "                \"Train AUC\": train_auc,\n",
    "                \"Train Sensitivity\": train_sensitivity,\n",
    "                \"Train Specificity\": train_specificity,\n",
    "                \"Max Test AUC\": np.max(test_AUCs),\n",
    "            }\n",
    "        )\n",
    "            \n",
    "        # Early stopping\n",
    "        if epoch > 75 and test_auc < 0.7:\n",
    "            early_stopping+=1\n",
    "            if early_stopping > 25 and test_auc < 0.6 and num_logged == 0:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "            if early_stopping > 50 and test_auc < 0.65 and num_logged == 0:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "            \n",
    "            if early_stopping > 75 and test_auc < 0.7 and num_logged <= 1:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "\n",
    "\n",
    "    # save test preds and probs\n",
    "    np.save(results_path + '//best_test_preds.npy', np.array(best_test_preds))\n",
    "    np.save(results_path + '//best_test_probs.npy', np.array(best_test_probs))\n",
    "\n",
    "    print('Batches mixed:', batches_mixed, 'out of', len(train_dataloader)*num_epochs, 'percentage:', batches_mixed/(len(train_dataloader)*num_epochs))\n",
    "    print(hyperparams)\n",
    "    print('Time taken:', perf_counter() - time_start)\n",
    "    # Wait for GPU to cool down after each model run\n",
    "    print(f\"Cooling down for 5 mins...\")\n",
    "    time.sleep(60*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc563e2d5373fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start sweep job.\n",
    "wandb.agent(sweep_id, function=main, count=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
